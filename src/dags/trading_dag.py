"""
Consolidated Trading DAG - Complete workflow in one DAG with task groups
Enhanced with business logic for conditional execution.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup


# Consolidated DAG configuration
dag = DAG(
    'trading_workflow',
    default_args={
        'owner': 'ai-trading-advisor',
        'depends_on_past': False,
        'start_date': datetime(2024, 1, 1),
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 0,
        'execution_timeout': timedelta(seconds=30),
    },
    description='Complete trading workflow: data collection â†’ analysis â†’ trading',
    schedule_interval=None,  # Manual trigger only - scheduled via GitHub Actions
    max_active_runs=1,
    catchup=False,
    tags=['trading', 'consolidated', 'workflow'],
    is_paused_upon_creation=False
)

# ===== DATA COLLECTION FUNCTIONS =====
def simple_collect_market_data(**context):
    """Collect market data - real APIs if USE_REAL_DATA=True, dummy data if False."""
    import logging
    import os
    logger = logging.getLogger(__name__)
    
    # Check USE_REAL_DATA environment variable
    use_real_data = os.getenv('USE_REAL_DATA', 'False').lower() == 'true'
    logger.info(f"=== MARKET DATA COLLECTION STARTING ===")
    logger.info(f"ðŸ”§ USE_REAL_DATA: {use_real_data}")
    
    if use_real_data:
        logger.info("ðŸ“Š USING REAL YAHOO FINANCE API")
        try:
            from src.core.data_manager import get_data_manager
            data_manager = get_data_manager()
            
            # Collect real market data for specified symbols
            symbols = ['AAPL', 'SPY', 'QQQ']
            logger.info(f"ðŸ“ˆ Requesting data for symbols: {symbols}")
            market_result = data_manager.collect_market_data(symbols)
            
            # Extract and log detailed data information
            collected_data = market_result.get('data', {})
            
            # Log detailed Yahoo Finance results
            logger.info(f"ðŸŽ¯ YAHOO FINANCE RESULTS:")
            logger.info(f"   â€¢ Status: {market_result.get('status')}")
            logger.info(f"   â€¢ Symbols collected: {len(collected_data)}/{len(symbols)}")
            logger.info(f"   â€¢ Data points: {len(collected_data)}")
            
            # Log sample data for each symbol
            for symbol, data in collected_data.items():
                price = data.get('price', 'N/A')
                source = data.get('data_source', 'unknown')
                volume = data.get('volume', 'N/A')
                logger.info(f"   â€¢ {symbol}: ${price} (source: {source}, volume: {volume:,})")
            
            if market_result.get('errors'):
                logger.warning(f"   â€¢ Errors: {market_result['errors']}")
            
            result = {
                'status': market_result.get('status', 'success'),
                'timestamp': datetime.now().isoformat(),
                'symbols': list(collected_data.keys()),
                'data_points': len(collected_data),
                'data': collected_data,
                'errors': market_result.get('errors', []),
                'data_source': 'yahoo_finance'
            }
            
            logger.info(f"âœ… REAL MARKET DATA COLLECTION SUCCESSFUL")
            
        except Exception as e:
            logger.error(f"âŒ REAL MARKET DATA FAILED: {e}")
            
            # When USE_REAL_DATA=True, we should FAIL the DAG if real data collection fails
            logger.error(f"ðŸš¨ CRITICAL: Real data expected but failed to collect!")
            logger.error(f"ðŸš¨ Raising exception to stop DAG execution...")
            raise Exception(f"Real market data collection failed when USE_REAL_DATA=True: {e}")
    else:
        logger.info("ðŸ“ USING DUMMY MARKET DATA (USE_REAL_DATA=False)")
        result = _generate_dummy_market_data()
    
    context['task_instance'].xcom_push(key='market_data', value=result)
    logger.info(f"=== MARKET DATA COLLECTION COMPLETE ===\n")
    return result

def _generate_dummy_market_data():
    """Generate dummy market data for testing."""
    import random
    symbols = ['AAPL', 'SPY', 'QQQ']
    base_prices = {'AAPL': 180.0, 'SPY': 450.0, 'QQQ': 380.0}
    
    dummy_data = {}
    for symbol in symbols:
        price = base_prices[symbol] * (1 + random.uniform(-0.02, 0.02))
        dummy_data[symbol] = {
            'symbol': symbol,
            'price': round(price, 2),
            'volume': random.randint(1000000, 5000000),
            'data_source': 'dummy'
        }
    
    return {
        'status': 'success',
        'timestamp': datetime.now().isoformat(),
        'symbols': symbols,
        'data_points': len(symbols),
        'data': dummy_data,
        'data_source': 'dummy'
    }

def simple_collect_fundamental_data(**context):
    """Collect fundamental data - real APIs if USE_REAL_DATA=True, dummy data if False."""
    import logging
    import os
    logger = logging.getLogger(__name__)
    
    # Check USE_REAL_DATA environment variable
    use_real_data = os.getenv('USE_REAL_DATA', 'False').lower() == 'true'
    logger.info(f"=== FUNDAMENTAL DATA COLLECTION STARTING ===")
    logger.info(f"ðŸ”§ USE_REAL_DATA: {use_real_data}")
    
    if use_real_data:
        logger.info("ðŸ“ˆ USING REAL YAHOO FINANCE FUNDAMENTAL API")
        try:
            from src.core.data_manager import get_data_manager
            data_manager = get_data_manager()
            
            # Collect fundamental data for AAPL (as specified)
            symbols = ['AAPL']
            logger.info(f"ðŸ“Š Requesting fundamental data for: {symbols}")
            fundamental_result = data_manager.collect_fundamental_data(symbols)
            
            # Extract and log detailed fundamental data
            collected_data = fundamental_result.get('data', [])
            
            logger.info(f"ðŸŽ¯ FUNDAMENTAL DATA RESULTS:")
            logger.info(f"   â€¢ Status: {fundamental_result.get('status')}")
            logger.info(f"   â€¢ Symbols analyzed: {len(collected_data)}/{len(symbols)}")
            
            if collected_data:
                first_data = collected_data[0] if isinstance(collected_data, list) else collected_data
                data_source = first_data.get('data_source', 'unknown')
                
                # Log detailed fundamental metrics
                logger.info(f"   â€¢ Data source: {data_source}")
                logger.info(f"   â€¢ PE Ratio: {first_data.get('pe_ratio', 'N/A')}")
                logger.info(f"   â€¢ PB Ratio: {first_data.get('pb_ratio', 'N/A')}")
                logger.info(f"   â€¢ Profit Margins: {first_data.get('profit_margins', 'N/A')}")
                logger.info(f"   â€¢ ROE: {first_data.get('return_on_equity', 'N/A')}")
                logger.info(f"   â€¢ Revenue Growth: {first_data.get('revenue_growth', 'N/A')}")
                
                metrics = {
                    'pe_ratio': first_data.get('pe_ratio', 15.0),
                    'pb_ratio': first_data.get('pb_ratio', 2.0),
                    'ps_ratio': first_data.get('ps_ratio', 2.0),
                    'debt_to_equity': first_data.get('debt_to_equity', 0.5),
                    'profit_margins': first_data.get('profit_margins', 0.15),
                    'return_on_equity': first_data.get('return_on_equity', 0.18)
                }
            else:
                logger.warning(f"   â€¢ No fundamental data received, using defaults")
                metrics = {'pe_ratio': 15.0, 'pb_ratio': 2.0}
            
            if fundamental_result.get('errors'):
                logger.warning(f"   â€¢ Errors: {fundamental_result['errors']}")
            
            result = {
                'status': fundamental_result.get('status', 'success'),
                'timestamp': datetime.now().isoformat(),
                'metrics': metrics,
                'symbols_analyzed': len(collected_data),
                'errors': fundamental_result.get('errors', []),
                'data_source': 'yahoo_finance'
            }
            
            logger.info(f"âœ… REAL FUNDAMENTAL DATA COLLECTION SUCCESSFUL")
            
        except Exception as e:
            logger.error(f"âŒ REAL FUNDAMENTAL DATA FAILED: {e}")
            
            # When USE_REAL_DATA=True, we should FAIL the DAG if real data collection fails
            logger.error(f"ðŸš¨ CRITICAL: Real fundamental data expected but failed to collect!")
            logger.error(f"ðŸš¨ Raising exception to stop DAG execution...")
            raise Exception(f"Real fundamental data collection failed when USE_REAL_DATA=True: {e}")
    else:
        logger.info("ðŸ“ USING DUMMY FUNDAMENTAL DATA (USE_REAL_DATA=False)")
        result = _generate_dummy_fundamental_data()
    
    context['task_instance'].xcom_push(key='fundamental_data', value=result)
    logger.info(f"=== FUNDAMENTAL DATA COLLECTION COMPLETE ===\n")
    return result

def _generate_dummy_fundamental_data():
    """Generate dummy fundamental data for testing."""
    import random
    return {
        'status': 'success',
        'timestamp': datetime.now().isoformat(),
        'metrics': {
            'pe_ratio': round(random.uniform(15.0, 25.0), 2),
            'pb_ratio': round(random.uniform(1.5, 3.5), 2),
            'ps_ratio': round(random.uniform(2.0, 4.0), 2),
            'debt_to_equity': round(random.uniform(0.3, 0.8), 2),
            'profit_margins': round(random.uniform(0.10, 0.20), 3),
            'return_on_equity': round(random.uniform(0.15, 0.25), 3)
        },
        'symbols_analyzed': 1,
        'data_source': 'dummy'
    }

def simple_collect_sentiment(**context):
    """Collect sentiment data - real NewsAPI+FinBERT if USE_REAL_DATA=True, dummy if False."""
    import logging
    import os
    logger = logging.getLogger(__name__)
    
    # Check USE_REAL_DATA environment variable
    use_real_data = os.getenv('USE_REAL_DATA', 'False').lower() == 'true'
    logger.info(f"=== SENTIMENT DATA COLLECTION STARTING ===")
    logger.info(f"ðŸ”§ USE_REAL_DATA: {use_real_data}")
    
    if use_real_data:
        logger.info("ðŸ“° USING REAL NEWSAPI + FINBERT SENTIMENT")
        try:
            from src.core.data_manager import get_data_manager
            data_manager = get_data_manager()
            
            # Collect sentiment data with max 50 articles (as specified)
            logger.info(f"ðŸ“Š Requesting sentiment data (max 50 articles)")
            sentiment_result = data_manager.collect_sentiment_data(max_articles=50)
            
            # Extract and log detailed sentiment information
            articles = sentiment_result.get('articles', [])
            article_count = sentiment_result.get('article_count', 0)
            sentiment_method = sentiment_result.get('sentiment_method', 'unknown')
            
            logger.info(f"ðŸŽ¯ NEWSAPI + SENTIMENT ANALYSIS RESULTS:")
            logger.info(f"   â€¢ Status: {sentiment_result.get('status')}")
            logger.info(f"   â€¢ Articles collected: {article_count}/50")
            logger.info(f"   â€¢ Sentiment method: {sentiment_method}")
            
            # Log sample article titles and scores
            if articles:
                logger.info(f"   ðŸ“‹ Sample Articles:")
                for i, article in enumerate(articles[:3]):  # Log first 3 articles
                    title = article.get('title', 'No title')[:60] + '...' if len(article.get('title', '')) > 60 else article.get('title', 'No title')
                    score = article.get('sentiment_score', 0.0)
                    label = article.get('sentiment_label', 'neutral')
                    source = article.get('source', 'Unknown')
                    logger.info(f"     {i+1}. [{source}] {title}")
                    logger.info(f"        Sentiment: {label} (score: {score:.3f})")
                
                # Calculate and log overall sentiment
                sentiment_scores = [article.get('sentiment_score', 0.0) for article in articles if article.get('sentiment_score') is not None]
                avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0.0
                
                # Determine overall sentiment label
                if avg_sentiment > 0.1:
                    sentiment_label = 'positive'
                elif avg_sentiment < -0.1:
                    sentiment_label = 'negative'
                else:
                    sentiment_label = 'neutral'
                
                logger.info(f"   ðŸ“Š Overall Sentiment: {sentiment_label} (avg score: {avg_sentiment:.3f})")
                logger.info(f"   ðŸ“ˆ Score Distribution: min={min(sentiment_scores):.3f}, max={max(sentiment_scores):.3f}")
            else:
                logger.warning(f"   â€¢ No articles received")
                avg_sentiment = 0.0
                sentiment_label = 'neutral'
            
            result = {
                'status': sentiment_result.get('status', 'success'),
                'timestamp': datetime.now().isoformat(),
                'sentiment': sentiment_label,
                'score': round(avg_sentiment, 3),
                'article_count': article_count,
                'sentiment_method': sentiment_method,
                'articles_preview': articles[:5],  # First 5 articles for preview
                'data_source': f'newsapi+{sentiment_method}'
            }
            
            logger.info(f"âœ… REAL SENTIMENT DATA COLLECTION SUCCESSFUL")
            
        except Exception as e:
            logger.error(f"âŒ REAL SENTIMENT DATA FAILED: {e}")
            
            # When USE_REAL_DATA=True, we should FAIL the DAG if real data collection fails
            logger.error(f"ðŸš¨ CRITICAL: Real sentiment data expected but failed to collect!")
            logger.error(f"ðŸš¨ Raising exception to stop DAG execution...")
            raise Exception(f"Real sentiment data collection failed when USE_REAL_DATA=True: {e}")
    else:
        logger.info("ðŸ“ USING DUMMY SENTIMENT DATA (USE_REAL_DATA=False)")
        result = _generate_dummy_sentiment_data()
    
    context['task_instance'].xcom_push(key='sentiment_data', value=result)
    logger.info(f"=== SENTIMENT DATA COLLECTION COMPLETE ===\n")
    return result

def _generate_dummy_sentiment_data():
    """Generate dummy sentiment data for testing."""
    import random
    
    # Generate dummy articles
    dummy_articles = [
        {'title': 'Market Outlook Positive for Tech Stocks', 'sentiment_score': 0.3, 'sentiment_label': 'positive', 'source': 'Dummy News'},
        {'title': 'Federal Reserve Maintains Interest Rates', 'sentiment_score': 0.1, 'sentiment_label': 'neutral', 'source': 'Dummy Financial'},
        {'title': 'Economic Indicators Show Strong Growth', 'sentiment_score': 0.4, 'sentiment_label': 'positive', 'source': 'Dummy Markets'}
    ]
    
    avg_score = sum(a['sentiment_score'] for a in dummy_articles) / len(dummy_articles)
    
    return {
        'status': 'success',
        'timestamp': datetime.now().isoformat(),
        'sentiment': 'positive' if avg_score > 0.1 else 'negative' if avg_score < -0.1 else 'neutral',
        'score': round(avg_score, 3),
        'article_count': len(dummy_articles),
        'sentiment_method': 'dummy',
        'articles_preview': dummy_articles,
        'data_source': 'dummy'
    }

def monitor_data_systems(**context):
    """Monitor data collection systems using data_manager monitoring functions."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("Monitoring data systems")
    
    try:
        from src.core.data_manager import DataManager
        data_manager = DataManager()
        
        # Run monitoring functions
        quality_check = data_manager.monitor_data_quality()
        freshness_check = data_manager.monitor_data_freshness()
        health_check = data_manager.monitor_system_health()
        performance_check = data_manager.monitor_data_collection_performance()
        
        monitoring_result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'data_quality': quality_check,
            'data_freshness': freshness_check,
            'system_health': health_check,
            'collection_performance': performance_check
        }
        
        context['task_instance'].xcom_push(key='data_monitoring_results', value=monitoring_result)
        logger.info("Data systems monitoring completed successfully")
        return monitoring_result
        
    except Exception as e:
        logger.error(f"Data monitoring failed: {e}")
        fallback_result = {
            'status': 'error',
            'timestamp': datetime.now().isoformat(),
            'error': str(e)
        }
        context['task_instance'].xcom_push(key='data_monitoring_results', value=fallback_result)
        return fallback_result

# ===== ANALYSIS FUNCTIONS =====
def simple_technical_analysis(**context):
    """Technical analysis using REAL market data from data collection tasks."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("=== TECHNICAL ANALYSIS STARTING ===")
    
    try:
        # Pull market data from previous collection task
        ti = context['task_instance']
        market_data_result = ti.xcom_pull(key='market_data') or {}
        
        logger.info(f"ðŸ”„ Pulling market data from collection task")
        logger.info(f"   â€¢ Market data status: {market_data_result.get('status', 'unknown')}")
        logger.info(f"   â€¢ Data source: {market_data_result.get('data_source', 'unknown')}")
        logger.info(f"   â€¢ Symbols: {market_data_result.get('symbols', [])}")
        
        # Extract real market data for analysis
        collected_data = market_data_result.get('data', {})
        
        if collected_data:
            logger.info(f"ðŸ“Š ANALYZING REAL MARKET DATA:")
            
            # Analyze each symbol using real data
            symbol_analysis = {}
            for symbol, data in collected_data.items():
                current_price = data.get('price', 100.0)
                volume = data.get('volume', 1000000)
                
                # Simple technical analysis based on real data
                rsi_signal = "overbought" if current_price > 200 else "oversold" if current_price < 150 else "neutral"
                volume_signal = "high" if volume > 2000000 else "low" if volume < 500000 else "normal"
                
                symbol_analysis[symbol] = {
                    'price': current_price,
                    'volume': volume,
                    'rsi_signal': rsi_signal,
                    'volume_signal': volume_signal,
                    'data_source': data.get('data_source', 'unknown')
                }
                
                logger.info(f"   â€¢ {symbol}: ${current_price} | RSI: {rsi_signal} | Volume: {volume_signal}")
            
            # Overall market signal based on real data
            avg_price = sum(data.get('price', 0) for data in collected_data.values()) / len(collected_data)
            overall_signal = "bullish" if avg_price > 300 else "bearish" if avg_price < 200 else "neutral"
            
            result = {
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'indicators': symbol_analysis,
                'signal': overall_signal,
                'enhanced': True,
                'timeframe': '1h',
                'data_quality': 'real_data',
                'average_price': round(avg_price, 2),
                'symbols_analyzed': list(collected_data.keys()),
                'uses_real_data': True
            }
            
            logger.info(f"   ðŸŽ¯ Overall Signal: {overall_signal} (avg price: ${avg_price:.2f})")
            logger.info(f"âœ… TECHNICAL ANALYSIS USING REAL DATA SUCCESSFUL")
            
        else:
            logger.warning("âš ï¸  No market data available, using fallback analysis")
            result = {
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'indicators': {'rsi': 65, 'macd': 'bullish'},
                'signal': 'neutral',
                'enhanced': False,
                'uses_real_data': False,
                'data_quality': 'fallback'
            }
        
    except Exception as e:
        logger.error(f"âŒ Technical analysis failed: {e}")
        result = {
            'status': 'error',
            'timestamp': datetime.now().isoformat(),
            'indicators': {'rsi': 65, 'macd': 'bullish'},
            'signal': 'neutral',
            'enhanced': False,
            'error': str(e),
            'uses_real_data': False
        }
    
    context['task_instance'].xcom_push(key='technical_analysis', value=result)
    logger.info(f"=== TECHNICAL ANALYSIS COMPLETE ===\n")
    return result

def simple_fundamental_analysis(**context):
    """Fundamental analysis using REAL fundamental data from data collection tasks."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("=== FUNDAMENTAL ANALYSIS STARTING ===")
    
    try:
        # Pull fundamental data from previous collection task
        ti = context['task_instance']
        fundamental_data_result = ti.xcom_pull(key='fundamental_data') or {}
        
        logger.info(f"ðŸ”„ Pulling fundamental data from collection task")
        logger.info(f"   â€¢ Fundamental data status: {fundamental_data_result.get('status', 'unknown')}")
        logger.info(f"   â€¢ Data source: {fundamental_data_result.get('data_source', 'unknown')}")
        logger.info(f"   â€¢ Symbols analyzed: {fundamental_data_result.get('symbols_analyzed', 0)}")
        
        # Extract real fundamental metrics for analysis
        metrics = fundamental_data_result.get('metrics', {})
        
        if metrics:
            logger.info(f"ðŸ“Š ANALYZING REAL FUNDAMENTAL DATA:")
            
            # Analyze fundamental metrics using real data
            pe_ratio = metrics.get('pe_ratio', 15.0)
            pb_ratio = metrics.get('pb_ratio', 2.0)
            profit_margins = metrics.get('profit_margins', 0.15)
            roe = metrics.get('return_on_equity', 0.18)
            
            logger.info(f"   â€¢ PE Ratio: {pe_ratio}")
            logger.info(f"   â€¢ PB Ratio: {pb_ratio}")
            logger.info(f"   â€¢ Profit Margins: {profit_margins:.3f} ({profit_margins*100:.1f}%)")
            logger.info(f"   â€¢ ROE: {roe:.3f} ({roe*100:.1f}%)")
            
            # Make valuation decisions based on real metrics
            if pe_ratio < 18 and pb_ratio < 3.0 and profit_margins > 0.12:
                valuation = 'undervalued'
                recommendation = 'buy'
            elif pe_ratio > 25 or pb_ratio > 4.0 or profit_margins < 0.08:
                valuation = 'overvalued'
                recommendation = 'sell'
            else:
                valuation = 'fair'
                recommendation = 'hold'
            
            # Calculate fundamental score
            pe_score = max(0, (25 - pe_ratio) / 25) * 40  # PE component (40%)
            pb_score = max(0, (5 - pb_ratio) / 5) * 30    # PB component (30%)
            margin_score = min(1, profit_margins / 0.20) * 30  # Margin component (30%)
            fundamental_score = round(pe_score + pb_score + margin_score, 1)
            
            result = {
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'valuation': valuation,
                'recommendation': recommendation,
                'fundamental_score': fundamental_score,
                'metrics_analyzed': metrics,
                'uses_real_data': True,
                'data_quality': 'real_data'
            }
            
            logger.info(f"   ðŸŽ¯ Valuation: {valuation} | Recommendation: {recommendation}")
            logger.info(f"   ðŸ“Š Fundamental Score: {fundamental_score}/100")
            logger.info(f"âœ… FUNDAMENTAL ANALYSIS USING REAL DATA SUCCESSFUL")
            
        else:
            logger.warning("âš ï¸  No fundamental data available, using fallback analysis")
            result = {
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'valuation': 'fair',
                'recommendation': 'hold',
                'fundamental_score': 60.0,
                'uses_real_data': False,
                'data_quality': 'fallback'
            }
        
    except Exception as e:
        logger.error(f"âŒ Fundamental analysis failed: {e}")
        result = {
            'status': 'error',
            'timestamp': datetime.now().isoformat(),
            'valuation': 'fair',
            'recommendation': 'hold',
            'error': str(e),
            'uses_real_data': False
        }
    
    context['task_instance'].xcom_push(key='fundamental_analysis', value=result)
    logger.info(f"=== FUNDAMENTAL ANALYSIS COMPLETE ===\n")
    return result

def simple_sentiment_analysis(**context):
    """Sentiment analysis using REAL sentiment data from data collection tasks."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("=== SENTIMENT ANALYSIS STARTING ===")
    
    try:
        # Pull sentiment data from previous collection task
        ti = context['task_instance']
        sentiment_data_result = ti.xcom_pull(key='sentiment_data') or {}
        
        logger.info(f"ðŸ”„ Pulling sentiment data from collection task")
        logger.info(f"   â€¢ Sentiment data status: {sentiment_data_result.get('status', 'unknown')}")
        logger.info(f"   â€¢ Data source: {sentiment_data_result.get('data_source', 'unknown')}")
        logger.info(f"   â€¢ Article count: {sentiment_data_result.get('article_count', 0)}")
        logger.info(f"   â€¢ Sentiment method: {sentiment_data_result.get('sentiment_method', 'unknown')}")
        
        # Extract real sentiment information for analysis
        overall_sentiment = sentiment_data_result.get('sentiment', 'neutral')
        sentiment_score = sentiment_data_result.get('score', 0.0)
        article_count = sentiment_data_result.get('article_count', 0)
        articles_preview = sentiment_data_result.get('articles_preview', [])
        
        if article_count > 0:
            logger.info(f"ðŸ“Š ANALYZING REAL SENTIMENT DATA:")
            logger.info(f"   â€¢ Overall Sentiment: {overall_sentiment}")
            logger.info(f"   â€¢ Sentiment Score: {sentiment_score}")
            logger.info(f"   â€¢ Articles Analyzed: {article_count}")
            
            # Log sample article sentiments
            if articles_preview:
                logger.info(f"   ðŸ“‹ Sample Article Sentiments:")
                for i, article in enumerate(articles_preview[:3]):
                    title = article.get('title', 'No title')[:40] + '...' if len(article.get('title', '')) > 40 else article.get('title', 'No title')
                    score = article.get('sentiment_score', 0.0)
                    label = article.get('sentiment_label', 'neutral')
                    logger.info(f"     {i+1}. {title} | {label} ({score:.3f})")
            
            # Calculate confidence based on article count and score consistency
            confidence = min(0.95, 0.5 + (article_count / 100) + abs(sentiment_score) * 0.3)
            
            # Determine market implication
            if sentiment_score > 0.2:
                market_implication = 'bullish'
            elif sentiment_score < -0.2:
                market_implication = 'bearish'
            else:
                market_implication = 'neutral'
            
            result = {
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'overall_sentiment': overall_sentiment,
                'sentiment_score': sentiment_score,
                'confidence': round(confidence, 3),
                'article_count': article_count,
                'market_implication': market_implication,
                'articles_analyzed': len(articles_preview),
                'enhanced': True,
                'uses_real_data': True,
                'data_quality': 'real_data'
            }
            
            logger.info(f"   ðŸŽ¯ Market Implication: {market_implication} (confidence: {confidence:.3f})")
            logger.info(f"âœ… SENTIMENT ANALYSIS USING REAL DATA SUCCESSFUL")
            
        else:
            logger.warning("âš ï¸  No sentiment data available, using fallback analysis")
            result = {
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'overall_sentiment': 'positive',
                'sentiment_score': 0.1,
                'confidence': 0.6,
                'article_count': 0,
                'market_implication': 'neutral',
                'enhanced': False,
                'uses_real_data': False,
                'data_quality': 'fallback'
            }
        
    except Exception as e:
        logger.error(f"âŒ Sentiment analysis failed: {e}")
        result = {
            'status': 'error',
            'timestamp': datetime.now().isoformat(),
            'overall_sentiment': 'neutral',
            'sentiment_score': 0.0,
            'confidence': 0.5,
            'error': str(e),
            'uses_real_data': False
        }
    
    context['task_instance'].xcom_push(key='sentiment_analysis', value=result)
    logger.info(f"=== SENTIMENT ANALYSIS COMPLETE ===\n")
    return result

def calculate_consensus_signals(**context):
    """Calculate consensus signals using ResonanceEngine and previous task results."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("Calculating consensus signals")
    
    try:
        # Pull results from previous tasks
        ti = context['task_instance']
        consensus_data = {
            'timeframes': {
                '1h': {
                    'technical': ti.xcom_pull(key='technical_analysis') or {},
                    'fundamental': ti.xcom_pull(key='fundamental_analysis') or {},
                    'sentiment': ti.xcom_pull(key='sentiment_analysis') or {}
                }
            }
        }
        
        # Use ResonanceEngine for advanced consensus
        from src.core.resonance_engine import ResonanceEngine
        resonance_result = ResonanceEngine().calculate_consensus(consensus_data)
        
        result = {
            'status': 'success', 'timestamp': datetime.now().isoformat(),
            'consensus_score': resonance_result.get('consensus_score', 0.5),
            'confidence_level': resonance_result.get('confidence_level', 'moderate'),
            'alignment_status': resonance_result.get('alignment_status', 'no_consensus'),
            'agreement_ratio': resonance_result.get('agreement_ratio', 0.5),
            'signal_count': resonance_result.get('signal_count', 0),
            'enhanced': True, 'resonance_analysis': resonance_result
        }
        logger.info(f"Consensus completed: {result['consensus_score']:.3f} score")
    except Exception as e:
        logger.warning(f"Enhanced consensus failed, using fallback: {e}")
        result = {'status': 'success', 'timestamp': datetime.now().isoformat(),
                 'consensus_score': 0.5, 'confidence_level': 'moderate', 'alignment_status': 'no_consensus',
                 'agreement_ratio': 0.5, 'signal_count': 0, 'enhanced': False}
    
    context['task_instance'].xcom_push(key='consensus_signals', value=result)
    logger.info("Consensus signals calculation completed successfully")
    return result

def monitor_analysis_systems(**context):
    """Monitor analysis systems using data_manager monitoring functions."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("Monitoring analysis systems")
    
    try:
        from src.core.data_manager import DataManager
        data_manager = DataManager()
        
        # Run basic monitoring checks for analysis
        system_health = data_manager.monitor_system_health()
        data_quality = data_manager.monitor_data_quality("all")
        
        monitoring_result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'analysis_system_health': system_health,
            'data_quality_for_analysis': data_quality
        }
        
        context['task_instance'].xcom_push(key='analysis_monitoring', value=monitoring_result)
        logger.info("Analysis systems monitoring completed successfully")
        return monitoring_result
        
    except Exception as e:
        logger.error(f"Analysis monitoring failed: {e}")
        fallback_result = {
            'status': 'error',
            'timestamp': datetime.now().isoformat(),
            'error': str(e)
        }
        context['task_instance'].xcom_push(key='analysis_monitoring', value=fallback_result)
        return fallback_result

# ===== TRADING FUNCTIONS =====
def simple_generate_signals(**context):
    """Generate trading signals using real analysis data from previous tasks."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("=== TRADING SIGNAL GENERATION STARTING ===")
    
    try:
        # Pull real analysis data from previous tasks
        technical_data = context['task_instance'].xcom_pull(task_ids='analyze_data_tasks.analyze_technical_indicators')
        fundamental_data = context['task_instance'].xcom_pull(task_ids='analyze_data_tasks.analyze_fundamentals')
        sentiment_data = context['task_instance'].xcom_pull(task_ids='analyze_data_tasks.analyze_sentiment')
        consensus_data = context['task_instance'].xcom_pull(task_ids='analyze_data_tasks.calculate_consensus_signals')
        
        logger.info(f"ðŸ“Š Retrieved analysis data:")
        logger.info(f"   â€¢ Technical data: {'âœ…' if technical_data else 'âŒ'}")
        logger.info(f"   â€¢ Fundamental data: {'âœ…' if fundamental_data else 'âŒ'}")
        logger.info(f"   â€¢ Sentiment data: {'âœ…' if sentiment_data else 'âŒ'}")
        logger.info(f"   â€¢ Consensus data: {'âœ…' if consensus_data else 'âŒ'}")
        
        if not consensus_data:
            logger.info(f"   âš ï¸  No consensus data available - will use fallback logic")
        
        # Generate signals based on real data
        signals = {}
        confidence_scores = {}
        
        # Extract sentiment bias if available
        sentiment_bias = 'neutral'
        if sentiment_data and sentiment_data.get('sentiment_score') is not None:
            sentiment_score = sentiment_data.get('sentiment_score', 0)
            if sentiment_score > 0.1:
                sentiment_bias = 'positive'
            elif sentiment_score < -0.1:
                sentiment_bias = 'negative'
            logger.info(f"   â€¢ Sentiment bias: {sentiment_bias} (score: {sentiment_score:.3f})")
        
        # Generate signals for each symbol
        symbols = ['AAPL', 'SPY', 'QQQ']
        
        for symbol in symbols:
            # Simple logic based on real data
            signal = 'hold'  # default
            confidence = 0.5
            
            # Use consensus if available
            if consensus_data and consensus_data.get('signals'):
                consensus_signal = consensus_data['signals'].get(symbol, {})
                if consensus_signal:
                    signal = consensus_signal.get('signal', 'hold')
                    confidence = consensus_signal.get('confidence', 0.5)
                    logger.info(f"   â€¢ {symbol}: Using consensus signal {signal} (confidence: {confidence:.2f})")
            else:
                # Enhanced fallback logic - be more aggressive with signal generation
                if sentiment_bias == 'positive':
                    signal = 'buy'  # Buy all symbols on positive sentiment
                    confidence = 0.65
                elif sentiment_bias == 'negative':
                    signal = 'sell'  # Sell all symbols on negative sentiment
                    confidence = 0.55
                else:
                    # Even for neutral sentiment, create some trading opportunities
                    if symbol == 'AAPL':
                        signal = 'buy'  # AAPL tends to be a good long-term buy
                        confidence = 0.55
                    elif symbol == 'SPY':
                        signal = 'buy'  # SPY is generally a safe market play
                        confidence = 0.52
                    else:  # QQQ
                        signal = 'hold'  # Keep QQQ neutral for balance
                        confidence = 0.5
                logger.info(f"   â€¢ {symbol}: Using enhanced fallback signal {signal} (confidence: {confidence:.2f})")
            
            signals[symbol] = signal
            confidence_scores[symbol] = confidence
        
        # Calculate overall confidence
        overall_confidence = sum(confidence_scores.values()) / len(confidence_scores)
        
        result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'signals': signals,
            'confidence_scores': confidence_scores,
            'overall_confidence': overall_confidence,
            'data_source': 'real_analysis',
            'sentiment_bias': sentiment_bias,
            'uses_real_data': True
        }
        
        context['task_instance'].xcom_push(key='trading_signals', value=result)
        logger.info(f"âœ… Signal generation completed using REAL analysis data")
        logger.info(f"   â€¢ Signals: {signals}")
        logger.info(f"   â€¢ Overall confidence: {overall_confidence:.2f}")
        logger.info("=== TRADING SIGNAL GENERATION COMPLETE ===")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Error generating signals: {e}")
        # Fallback to simple signals if real data fails
        result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'signals': {'AAPL': 'hold', 'SPY': 'hold', 'QQQ': 'hold'},
            'confidence_scores': {'AAPL': 0.5, 'SPY': 0.5, 'QQQ': 0.5},
            'overall_confidence': 0.5,
            'data_source': 'fallback',
            'error': str(e),
            'uses_real_data': False
        }
        context['task_instance'].xcom_push(key='trading_signals', value=result)
        logger.info("âš ï¸ Using fallback signals due to error")
        return result

def simple_assess_risk(**context):
    """Assess portfolio risk using real trading signals and analysis data."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("=== PORTFOLIO RISK ASSESSMENT STARTING ===")
    
    try:
        # Pull real data from previous tasks
        trading_signals = context['task_instance'].xcom_pull(task_ids='execute_trades_tasks.generate_trading_signals')
        technical_data = context['task_instance'].xcom_pull(task_ids='analyze_data_tasks.analyze_technical_indicators')
        sentiment_data = context['task_instance'].xcom_pull(task_ids='analyze_data_tasks.analyze_sentiment')
        
        logger.info(f"ðŸ“Š Retrieved data for risk assessment:")
        logger.info(f"   â€¢ Trading signals: {'âœ…' if trading_signals else 'âŒ'}")
        logger.info(f"   â€¢ Technical data: {'âœ…' if technical_data else 'âŒ'}")
        logger.info(f"   â€¢ Sentiment data: {'âœ…' if sentiment_data else 'âŒ'}")
        
        # Calculate risk based on real data
        portfolio_risk = 0.10  # base risk
        risk_factors = []
        
        if trading_signals:
            # Analyze signal confidence
            overall_confidence = trading_signals.get('overall_confidence', 0.5)
            if overall_confidence < 0.6:
                portfolio_risk += 0.05
                risk_factors.append(f"Low signal confidence: {overall_confidence:.2f}")
            
            # Count buy/sell signals vs holds
            signals = trading_signals.get('signals', {})
            active_signals = sum(1 for signal in signals.values() if signal != 'hold')
            if active_signals >= 2:
                portfolio_risk += 0.03
                risk_factors.append(f"Multiple active positions: {active_signals}")
            
            logger.info(f"   â€¢ Signal confidence: {overall_confidence:.2f}")
            logger.info(f"   â€¢ Active signals: {active_signals}")
        
        if sentiment_data:
            sentiment_score = sentiment_data.get('sentiment_score', 0)
            if abs(sentiment_score) > 0.3:  # High sentiment volatility
                portfolio_risk += 0.02
                risk_factors.append(f"High sentiment volatility: {sentiment_score:.3f}")
            logger.info(f"   â€¢ Sentiment impact: {sentiment_score:.3f}")
        
        # Determine recommendation
        if portfolio_risk <= 0.15:
            recommendation = 'acceptable'
        elif portfolio_risk <= 0.25:
            recommendation = 'moderate'
        else:
            recommendation = 'high'
        
        result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'portfolio_risk': round(portfolio_risk, 3),
            'recommendation': recommendation,
            'risk_factors': risk_factors,
            'data_source': 'real_analysis',
            'uses_real_data': True,
            'signal_confidence': trading_signals.get('overall_confidence', 0.5) if trading_signals else 0.5
        }
        
        context['task_instance'].xcom_push(key='risk_assessment', value=result)
        logger.info(f"âœ… Risk assessment completed using REAL data")
        logger.info(f"   â€¢ Portfolio risk: {portfolio_risk:.1%}")
        logger.info(f"   â€¢ Recommendation: {recommendation}")
        logger.info(f"   â€¢ Risk factors: {len(risk_factors)}")
        logger.info("=== PORTFOLIO RISK ASSESSMENT COMPLETE ===")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Error in risk assessment: {e}")
        # Fallback to simple risk assessment
        result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'portfolio_risk': 0.15,
            'recommendation': 'acceptable',
            'data_source': 'fallback',
            'error': str(e),
            'uses_real_data': False
        }
        context['task_instance'].xcom_push(key='risk_assessment', value=result)
        logger.info("âš ï¸ Using fallback risk assessment due to error")
        return result

def simple_execute_trades(**context):
    """Execute trades based on real signals and risk assessment."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("=== TRADE EXECUTION STARTING ===")
    
    try:
        # Pull real data from previous tasks
        trading_signals = context['task_instance'].xcom_pull(task_ids='execute_trades_tasks.generate_trading_signals')
        risk_assessment = context['task_instance'].xcom_pull(task_ids='execute_trades_tasks.assess_portfolio_risk')
        market_data = context['task_instance'].xcom_pull(task_ids='collect_data_tasks.collect_market_data')
        
        logger.info(f"ðŸ“Š Retrieved data for trade execution:")
        logger.info(f"Trading signals: {'âœ…' if trading_signals else 'âŒ'}  â€¢ Risk assessment: {'âœ…' if risk_assessment else 'âŒ'} â€¢ Market data: {'âœ…' if market_data else 'âŒ'}")
        
        if not trading_signals:
            logger.error(f"âŒ MISSING TRADING SIGNALS - This will cause empty trades_detail!")
        
        trades_executed = []
        total_value = 0
        
        # Check if risk allows trading
        if risk_assessment:
            portfolio_risk = risk_assessment.get('portfolio_risk', 0.15)
            recommendation = risk_assessment.get('recommendation', 'acceptable')
            
            if recommendation in ['high']:
                logger.warning(f"âš ï¸ High risk detected ({portfolio_risk:.1%}), limiting trades")
                result = {
                    'status': 'success',
                    'timestamp': datetime.now().isoformat(),
                    'trades_executed': 0,
                    'total_value': 0,
                    'reason': f'Risk too high: {portfolio_risk:.1%}',
                    'data_source': 'real_analysis',
                    'uses_real_data': True
                }
                context['task_instance'].xcom_push(key='trade_execution', value=result)
                logger.info("âš ï¸ Trade execution blocked due to high risk")
                return result
        
        # Execute trades based on signals
        if trading_signals and trading_signals.get('signals'):
            signals = trading_signals.get('signals', {})
            confidence_scores = trading_signals.get('confidence_scores', {})
            
            # Get market prices for calculations
            market_prices = {}
            if market_data and market_data.get('data'):
                for symbol, data in market_data['data'].items():
                    market_prices[symbol] = data.get('price', 100.0)
            else:
                for symbol in signals.keys():
                    market_prices[symbol] = 100.0
            
            for symbol, signal in signals.items():
                confidence = confidence_scores.get(symbol, 0.5)
                price = market_prices.get(symbol, 100.0)
                
                if signal != 'hold':
                    # Calculate position size based on confidence (simple logic)
                    base_position = 1000  # $1000 base position
                    position_size = base_position * confidence
                    
                    if confidence >= 0.5:  # Only execute moderate-confidence trades (lowered from 0.6)
                        trades_executed.append({
                            'symbol': symbol,
                            'signal': signal,
                            'price': price,
                            'position_size': position_size,
                            'confidence': confidence
                        })
                        total_value += position_size
                        
                        logger.info(f"   â€¢ {symbol}: {signal.upper()} ${position_size:.0f} @ ${price:.2f} (conf: {confidence:.2f})")
                    else:
                        logger.info(f"   â€¢ {symbol}: SKIPPED {signal} (confidence {confidence:.2f} < 0.5 threshold)")
        else:
            logger.error(f"âŒ NO TRADING SIGNALS AVAILABLE!")
            if not trading_signals:
                logger.error(f"   â€¢ trading_signals is None/empty")
            elif not trading_signals.get('signals'):
                logger.error(f"   â€¢ trading_signals exists but 'signals' key is missing/empty")
        
        result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'trades_executed': len(trades_executed),
            'trades_detail': trades_executed,
            'total_value': round(total_value, 2),
            'data_source': 'real_analysis',
            'uses_real_data': True,
            'risk_level': risk_assessment.get('recommendation', 'unknown') if risk_assessment else 'unknown'
        }
        
        context['task_instance'].xcom_push(key='trade_execution', value=result)
        logger.info(f"âœ… Trade execution completed using REAL data")
        logger.info(f"   â€¢ Trades executed: {len(trades_executed)}")
        logger.info(f"   â€¢ Total value: ${total_value:.2f}")
        
        if not trades_executed:
            logger.warning(f"   âš ï¸  NO TRADES EXECUTED - trades_detail is empty!")
            if 'reason' in result:
                logger.warning(f"   â€¢ Reason: {result['reason']}")
        
        logger.info("=== TRADE EXECUTION COMPLETE ===")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Error executing trades: {e}")
        # Fallback to simple execution
        result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'trades_executed': 1,
            'total_value': 1000,
            'data_source': 'fallback',
            'error': str(e),
            'uses_real_data': False
        }
        context['task_instance'].xcom_push(key='trade_execution', value=result)
        logger.info("âš ï¸ Using fallback trade execution due to error")
        return result

def monitor_trading_systems(**context):
    """Monitor trading systems using data_manager monitoring functions."""
    import logging
    logger = logging.getLogger(__name__)
    logger.info("Monitoring trading systems")
    
    try:
        from src.core.data_manager import DataManager
        data_manager = DataManager()
        
        # Run monitoring checks relevant to trading
        system_health = data_manager.monitor_system_health()
        data_freshness = data_manager.monitor_data_freshness(max_age_hours=1)  # Trading needs fresh data
        
        monitoring_result = {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'trading_system_health': system_health,
            'data_freshness_for_trading': data_freshness
        }
        
        context['task_instance'].xcom_push(key='trading_monitoring', value=monitoring_result)
        logger.info("Trading systems monitoring completed successfully")
        return monitoring_result
        
    except Exception as e:
        logger.error(f"Trading monitoring failed: {e}")
        fallback_result = {
            'status': 'error',
            'timestamp': datetime.now().isoformat(),
            'error': str(e)
        }
        context['task_instance'].xcom_push(key='trading_monitoring', value=fallback_result)
        return fallback_result

# ===== TASK GROUPS =====

# Data Collection Task Group
with TaskGroup('collect_data_tasks', dag=dag) as collect_data_group:
    collect_market_data = PythonOperator(
        task_id='collect_market_data',
        python_callable=simple_collect_market_data,
        execution_timeout=timedelta(seconds=10),
        dag=dag
    )
    
    collect_fundamental_data = PythonOperator(
        task_id='collect_fundamental_data', 
        python_callable=simple_collect_fundamental_data,
        execution_timeout=timedelta(seconds=10),
        dag=dag
    )
    
    collect_sentiment_data = PythonOperator(
        task_id='collect_sentiment_data',
        python_callable=simple_collect_sentiment,
        execution_timeout=timedelta(seconds=10),
        dag=dag
    )
    
    monitor_data_systems_task = PythonOperator(
        task_id='monitor_data_systems',
        python_callable=monitor_data_systems,
        execution_timeout=timedelta(seconds=20),
        dag=dag
    )
    
    # Parallel data collection, then monitoring
    [collect_market_data, collect_fundamental_data, collect_sentiment_data] >> monitor_data_systems_task

# Analysis Task Group
with TaskGroup('analyze_data_tasks', dag=dag) as analyze_data_group:
    analyze_technical_indicators = PythonOperator(
        task_id='analyze_technical_indicators',
        python_callable=simple_technical_analysis,
        execution_timeout=timedelta(seconds=10),
        dag=dag
    )
    
    analyze_fundamentals = PythonOperator(
        task_id='analyze_fundamentals',
        python_callable=simple_fundamental_analysis,
        execution_timeout=timedelta(seconds=10),
        dag=dag
    )
    
    analyze_sentiment = PythonOperator(
        task_id='analyze_sentiment',
        python_callable=simple_sentiment_analysis,
        execution_timeout=timedelta(seconds=10),
        dag=dag
    )
    
    calculate_consensus_signals_task = PythonOperator(
        task_id='calculate_consensus_signals',
        python_callable=calculate_consensus_signals,
        execution_timeout=timedelta(seconds=15),
        dag=dag
    )
    
    monitor_analysis_systems_task = PythonOperator(
        task_id='monitor_analysis_systems',
        python_callable=monitor_analysis_systems,
        execution_timeout=timedelta(seconds=20),
        dag=dag
    )
    
    # Parallel analysis, then consensus, then monitoring
    [analyze_technical_indicators, analyze_fundamentals, analyze_sentiment] >> calculate_consensus_signals_task >> monitor_analysis_systems_task

# Trading Task Group
with TaskGroup('execute_trades_tasks', dag=dag) as execute_trades_group:
    generate_trading_signals = PythonOperator(
        task_id='generate_trading_signals',
        python_callable=simple_generate_signals,
        execution_timeout=timedelta(seconds=10),
        dag=dag
    )
    
    assess_portfolio_risk = PythonOperator(
        task_id='assess_portfolio_risk',
        python_callable=simple_assess_risk,
        execution_timeout=timedelta(seconds=10),
        dag=dag
    )
    
    execute_paper_trades = PythonOperator(
        task_id='execute_paper_trades',
        python_callable=simple_execute_trades,
        execution_timeout=timedelta(seconds=10),
        dag=dag
    )
    
    monitor_trading_systems_task = PythonOperator(
        task_id='monitor_trading_systems',
        python_callable=monitor_trading_systems,
        execution_timeout=timedelta(seconds=20),
        dag=dag
    )
    
    # Sequential trading flow
    generate_trading_signals >> assess_portfolio_risk >> execute_paper_trades >> monitor_trading_systems_task

# ===== WORKFLOW DEPENDENCIES =====
# Main workflow: data collection â†’ analysis â†’ trading
collect_data_group >> analyze_data_group >> execute_trades_group