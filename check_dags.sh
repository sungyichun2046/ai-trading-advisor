#!/bin/bash
# ------------------------------------------------------------------
# Test Consolidated Trading DAG - Single DAG with Task Groups
#
# Steps:
#   1. Test local DAG import (single consolidated DAG)
#   2. Start production environment: docker compose up -d
#   3. Trigger single trading_workflow DAG
#   4. Validate task group execution: collect_data_tasks â†’ analyze_data_tasks â†’ execute_trades_tasks
#   5. Check task completion and workflow success
#
# Uses docker-compose.yml with AIRFLOW_PORT=8081 (test environment)
# Mounts ./src/dags â†’ /opt/airflow/dags
#
# Usage:
#   ./check_dags.sh                    # Default: Test consolidated DAG workflow
#   ./check_dags.sh --timeout=N        # Set custom timeout
#
# Consolidated DAG Mode:
#   â€¢ Single trading_workflow DAG with 3 task groups
#   â€¢ Native Airflow task dependencies (no ExternalTaskSensor)
#   â€¢ Sequential execution: collect_data_tasks â†’ analyze_data_tasks â†’ execute_trades_tasks
#   â€¢ Unified workflow validation
#

set -e

# Parse command line arguments
WAIT_TIMEOUT=120           # 2 minutes default for single DAG execution
REAL_DATA_MODE=false       # Flag for real data testing

# Check for command line options
for arg in "$@"; do
    case $arg in
        --timeout=*)
            WAIT_TIMEOUT="${arg#*=}"
            echo "â±ï¸  Wait timeout set to: ${WAIT_TIMEOUT}s"
            ;;
        --real-data)
            REAL_DATA_MODE=true
            echo "ğŸ”´ Real data mode enabled"
            ;;
        --help)
            echo "Usage:"
            echo "  ./check_dags.sh                # Default: Test consolidated trading DAG"
            echo "  ./check_dags.sh --timeout=N    # Set custom timeout in seconds"
            echo "  ./check_dags.sh --real-data    # Enable real API data collection"
            echo ""
            echo "Tests single consolidated trading_workflow DAG with task groups:"
            echo "  â€¢ collect_data_tasks (parallel data collection)"
            echo "  â€¢ analyze_data_tasks (analysis with consensus)"
            echo "  â€¢ execute_trades_tasks (trading execution)"
            echo ""
            echo "Real Data Mode:"
            echo "  â€¢ Uses Yahoo Finance API for market data"
            echo "  â€¢ Uses NewsAPI for sentiment data"
            echo "  â€¢ Uses FinBERT for sentiment analysis"
            exit 0
            ;;
    esac
done

if [ "$REAL_DATA_MODE" = true ]; then
    echo "ğŸš€ Mode: Real Data Integration Validation"
    echo "=========================================="
    echo "ğŸ”´ REAL DATA MODE ENABLED"
    echo "  â€¢ Yahoo Finance API: Market data for AAPL, SPY, QQQ"
    echo "  â€¢ NewsAPI: Sentiment data (max 50 articles)"
    echo "  â€¢ FinBERT: Advanced sentiment analysis"
else
    echo "ğŸš€ Mode: Consolidated Trading DAG Validation"
    echo "============================================"
    echo "ğŸŸ¢ DUMMY DATA MODE (default)"
fi
echo ""

# Set environment for testing (same database, different port)
export POSTGRES_HOST=localhost
export POSTGRES_DB=airflow
export POSTGRES_USER=airflow
export POSTGRES_PASSWORD=airflow

# Configure data collection mode
if [ "$REAL_DATA_MODE" = true ]; then
    export USE_REAL_DATA=True
    export NEWSAPI_KEY=494b17bf8af14d7cbb2d62f1e8b11088
    echo "ğŸ”´ Environment configured for REAL DATA collection"
else
    export USE_REAL_DATA=False
    echo "ğŸŸ¢ Environment configured for DUMMY DATA collection"
fi

echo "ğŸ“ Test DAG Folder: $(pwd)/src/dags"
echo "ğŸ“ Expected: Single trading_dag.py with task groups"
echo "ğŸ³ Using Docker environment (docker-compose.yml, port 8081)"
echo ""

# Check if source dags folder exists
if [ ! -d "src/dags" ]; then
    echo "âŒ ERROR: src/dags/ folder not found!"
    echo "   Expected consolidated DAG structure not present"
    exit 1
fi

echo "ğŸ” SCANNING CONSOLIDATED DAG STRUCTURE"
echo "======================================"

# List Python files in dags folder
dag_files=$(find src/dags -name "*.py" -not -name "__*" 2>/dev/null || echo "")

if [ -z "$dag_files" ]; then
    echo "âŒ ERROR: No Python DAG files found in src/dags/"
    exit 1
fi

echo "ğŸ“‹ Found DAG files:"
for file in $dag_files; do
    echo "   - $file"
done

# Validate we have exactly one DAG file
dag_count=$(echo "$dag_files" | wc -l)
if [ "$dag_count" -eq 1 ]; then
    echo "âœ… PERFECT: Found exactly 1 consolidated DAG file"
else
    echo "âŒ ERROR: Expected 1 consolidated DAG, found $dag_count files"
    echo "   Consolidation incomplete!"
    exit 1
fi
echo ""

# Check for trading utilities
echo "ğŸ”— TRADING UTILITIES VALIDATION"
echo "==============================="

if [ ! -f "src/utils/trading_utils.py" ]; then
    echo "âŒ ERROR: src/utils/trading_utils.py not found!"
    echo "   Trading utilities file is missing"
    exit 1
fi
echo "âœ… Trading utilities: src/utils/trading_utils.py found"

# Test trading utilities import
echo ""
echo "ğŸ§ª Testing trading utilities..."
utils_test=$(POSTGRES_HOST=localhost POSTGRES_DB=airflow POSTGRES_USER=airflow POSTGRES_PASSWORD=airflow venv/bin/python -c "
import sys
sys.path.append('$(pwd)')
try:
    from src.utils.trading_utils import (
        is_market_open, safe_to_trade, should_run_analysis, 
        data_collection_branch_function, analysis_branch_function, trading_branch_function
    )
    print('âœ… Trading utilities import successful')
    
    # Test basic functions
    market_status = is_market_open()
    print(f'âœ… Market status check: {market_status}')
    
    trading_safe = safe_to_trade()
    print(f'âœ… Trading safety check: {trading_safe}')
    
    analysis_ok = should_run_analysis()
    print(f'âœ… Analysis readiness check: {analysis_ok}')
    
    print('âœ… All trading utilities working correctly')
    
except ImportError as e:
    print(f'âŒ Import failed: {e}')
    exit(1)
except Exception as e:
    print(f'âŒ Trading utilities test failed: {e}')
    exit(1)
" 2>&1)

if [ $? -eq 0 ]; then
    echo "$utils_test"
else
    echo "âŒ TRADING UTILITIES VALIDATION FAILED"
    echo "$utils_test"
    exit 1
fi
echo ""

echo "âœ… TRADING UTILITIES VALIDATION COMPLETE"
echo "========================================"
echo ""

echo "ğŸ§ª TESTING CONSOLIDATED DAG IMPORT (LOCAL)"
echo "=========================================="

# Test the consolidated DAG file import locally
dag_file=$(echo "$dag_files" | head -1)
echo "Testing: $dag_file"

# Test Python import
python_test=$(venv/bin/python -c "
import sys
sys.path.append('$(pwd)')
try:
    module_path = '$dag_file'.replace('/', '.').replace('.py', '')
    exec(f'from {module_path} import dag')
    print(f'âœ… IMPORT SUCCESS: {dag.dag_id} ({len(dag.tasks)} tasks)')
    print(f'   Schedule: {dag.schedule_interval}')
    print(f'   Description: {dag.description}')
    
    # Validate task groups
    task_groups = []
    task_ids = [t.task_id for t in dag.tasks]
    for task in dag.tasks:
        if hasattr(task, 'task_group') and task.task_group:
            group_id = task.task_group.group_id
            if group_id not in task_groups:
                task_groups.append(group_id)
    
    print(f'   Task Groups: {task_groups}')
    print(f'   Total Tasks: {len(task_ids)}')
    
    # Check for expected task groups
    expected_groups = ['collect_data_tasks', 'analyze_data_tasks', 'execute_trades_tasks']
    for group in expected_groups:
        if group in task_groups:
            print(f'   âœ… {group}: Found')
        else:
            print(f'   âŒ {group}: Missing')
            
except Exception as e:
    print(f'âŒ IMPORT ERROR: {e}')
    exit(1)
" 2>&1)

if [ $? -eq 0 ]; then
    echo "$python_test"
else
    echo "âŒ FAILED: $dag_file"
    echo "$python_test"
    exit 1
fi
echo ""

echo "ğŸ“Š CONSOLIDATED DAG STRUCTURE VALIDATION"
echo "========================================"
echo "âœ… Single consolidated DAG structure complete"
echo "âœ… Task groups replace separate DAGs"
echo "âœ… No ExternalTaskSensor dependencies needed"
echo ""

echo "ğŸ³ STARTING DOCKER ENVIRONMENT (PORT 8081)"
echo "=========================================="

echo "ğŸ›‘ Stopping any running services..."
docker compose down 2>/dev/null || true

echo "ğŸš€ Starting Airflow environment on port 8081..."
export AIRFLOW_PORT=8081
docker compose up -d

echo "â³ Waiting for Airflow to initialize (port 8081)..."
sleep 60

# Wait for Airflow to be ready
echo "ğŸ”„ Checking Airflow health (port 8081)..."
max_attempts=15
attempt=0

while [ $attempt -lt $max_attempts ]; do
    health_check=$(curl -s http://localhost:8081/health 2>/dev/null || echo "failed")
    web_access=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8081 2>/dev/null || echo "000")
    
    # Accept both 200 and 302 as valid (302 is redirect to login page)
    if [[ "$health_check" != "failed" ]] && ([[ "$web_access" == "200" ]] || [[ "$web_access" == "302" ]]); then
        echo "âœ… Airflow is ready!"
        echo "   Health endpoint: âœ… http://localhost:8081/health"
        echo "   Web interface: âœ… http://localhost:8081 (HTTP $web_access)"
        break
    fi
    
    attempt=$((attempt + 1))
    echo "   Attempt $attempt/$max_attempts (Health: $health_check, Web: HTTP $web_access)..."
    sleep 15
done

if [ $attempt -eq $max_attempts ]; then
    echo "âŒ Airflow not ready after $max_attempts attempts"
    exit 1
fi

echo ""
echo "ğŸ” CRITICAL: CHECKING DAG IMPORT ERRORS"
echo "======================================="

# Check for DAG import errors - EARLY STOP if any found
echo "ğŸ§ª Checking for DAG import errors..."
import_errors=$(docker compose exec airflow-webserver airflow dags list-import-errors 2>/dev/null)

if [[ "$import_errors" == *"No data found"* ]]; then
    echo "âœ… SUCCESS: No DAG import errors found!"
    echo "âœ… Consolidated DAG loaded without issues"
else
    echo "âŒ CRITICAL ERROR: DAG IMPORT FAILURES DETECTED"
    echo "==============================================="
    echo ""
    echo "Import errors found:"
    echo "$import_errors"
    echo ""
    echo "âŒ EARLY STOP: Cannot proceed with DAG import errors"
    echo "âŒ Fix the import errors before continuing validation"
    exit 1
fi

# Verify consolidated DAG is loaded
echo ""
echo "ğŸ“‹ Verifying consolidated DAG..."
loaded_dags=$(docker compose exec airflow-webserver airflow dags list 2>/dev/null | grep "trading_workflow" | wc -l)

if [ "$loaded_dags" -eq 1 ]; then
    echo "âœ… SUCCESS: Consolidated trading_workflow DAG is loaded"
    dag_details=$(docker compose exec airflow-webserver airflow dags list 2>/dev/null | grep "trading_workflow")
    echo "Loaded DAG:"
    echo "$dag_details"
else
    echo "âŒ ERROR: Expected 1 trading_workflow DAG, found $loaded_dags"
    echo "âŒ EARLY STOP: Consolidated DAG not found"
    exit 1
fi

echo ""
echo "ğŸš€ CONSOLIDATED DAG EXECUTION TEST"
echo "=================================="

# Unpause the consolidated DAG
echo "ğŸ“‹ Unpausing trading_workflow DAG..."
docker compose exec airflow-webserver airflow dags unpause trading_workflow > /dev/null 2>&1
echo "âœ… trading_workflow DAG unpaused"

echo ""
echo "ğŸ”¥ Triggering consolidated trading_workflow DAG..."
execution_date=$(date -u '+%Y-%m-%dT%H:%M:%S')
echo "ğŸ“… Using execution_date: $execution_date"

# Trigger the consolidated DAG
docker compose exec airflow-webserver airflow dags trigger trading_workflow -e "$execution_date" > /dev/null 2>&1
echo "âœ… trading_workflow DAG triggered"

echo ""
if [ "$REAL_DATA_MODE" = true ]; then
    echo "â³ Waiting ${WAIT_TIMEOUT}s for REAL DATA workflow to complete..."
    echo "ğŸ“Š Expected execution with REAL APIs:"
    echo "     â€¢ collect_data_tasks: Yahoo Finance + NewsAPI + FinBERT"
    echo "     â€¢ analyze_data_tasks: Real data analysis + consensus"
    echo "     â€¢ execute_trades_tasks: Trading based on real data"
else
    echo "â³ Waiting ${WAIT_TIMEOUT}s for consolidated workflow to complete..."
    echo "ğŸ“Š Expected execution: collect_data_tasks â†’ analyze_data_tasks â†’ execute_trades_tasks"
fi
echo ""

# Wait for execution
sleep $WAIT_TIMEOUT

echo ""
echo "ğŸ“Š CONSOLIDATED DAG EXECUTION RESULTS"
echo "===================================="

# Check execution status
echo "ğŸ• Checking trading_workflow execution..."
workflow_runs=$(docker compose exec airflow-webserver \
    airflow dags list-runs -d trading_workflow 2>/dev/null \
    | grep -E "(success|running|failed)" | tr -d '\r' || echo "")

if [ -n "$workflow_runs" ]; then
    echo "ğŸ“‹ Recent workflow runs:"
    echo "$workflow_runs" | head -5
    
    # Count different states
    success_count=$(echo "$workflow_runs" | grep -c "success" || echo "0")
    running_count=$(echo "$workflow_runs" | grep -c "running" || echo "0")
    failed_count=$(echo "$workflow_runs" | grep -c "failed" || echo "0")
    
    echo ""
    echo "ğŸ“Š Execution Summary:"
    echo "   âœ… Success: $success_count runs"
    echo "   ğŸ”„ Running: $running_count runs" 
    echo "   âŒ Failed:  $failed_count runs"
    
else
    echo "âŒ No workflow runs found"
    success_count=0
    running_count=0
    failed_count=0
fi

echo ""
echo "ğŸ¯ FINAL VALIDATION RESULT"
echo "=========================="

# Determine overall success
if [ "$success_count" -gt 0 ]; then
    echo "ğŸ‰ âœ… SUCCESS: Consolidated trading_workflow completed successfully!"
    echo "âœ… Task groups executed in correct sequence"
    echo "âœ… No ExternalTaskSensor complexity needed" 
    echo "âœ… Single DAG workflow validation complete"
    final_result="SUCCESS"
elif [ "$running_count" -gt 0 ]; then
    echo "ğŸ”„ PARTIAL: trading_workflow is still running"
    echo "â³ DAG execution in progress but not completed within timeout"
    final_result="RUNNING"
else
    echo "âŒ FAILURE: trading_workflow did not complete successfully"
    echo "âŒ Check DAG execution details in Airflow UI"
    final_result="FAILURE"
fi

echo ""
echo "==============================================="
if [ "$REAL_DATA_MODE" = true ]; then
    echo "ğŸ¯ REAL DATA INTEGRATION VALIDATION REPORT"
else
    echo "ğŸ¯ CONSOLIDATED DAG VALIDATION REPORT"
fi
echo "==============================================="
echo ""
echo "ğŸ“ˆ Consolidated Structure:  âœ… Single trading_dag.py with task groups"
echo "ğŸ”— Task Group Dependencies: âœ… collect_data_tasks â†’ analyze_data_tasks â†’ execute_trades_tasks"
echo "ğŸš« ExternalTaskSensor:      âœ… Eliminated (native task dependencies)"
if [ "$REAL_DATA_MODE" = true ]; then
    echo "ğŸ”´ Data Integration:        âœ… Real API calls (Yahoo Finance + NewsAPI + FinBERT)"
else
    echo "ğŸŸ¢ Data Mode:               âœ… Dummy data (fast validation)"
fi
echo "ğŸ“Š Execution Result:        $final_result"
echo ""

if [ "$final_result" == "SUCCESS" ]; then
    if [ "$REAL_DATA_MODE" = true ]; then
        echo "ğŸ‰ OVERALL RESULT: âœ… SUCCESS - Real Data Integration complete!"
        echo "âœ… Yahoo Finance API: Market data collected successfully"
        echo "âœ… NewsAPI: Sentiment data collected successfully" 
        echo "âœ… FinBERT: Advanced sentiment analysis working"
        echo "âœ… Real data workflow execution validated"
    else
        echo "ğŸ‰ OVERALL RESULT: âœ… SUCCESS - Consolidated DAG workflow complete!"
        echo "âœ… Task group execution validated"
        echo "âœ… Single DAG architecture working perfectly"
    fi
    echo "âœ… No cross-DAG dependency issues"
elif [ "$final_result" == "RUNNING" ]; then
    echo "â³ OVERALL RESULT: ğŸŸ¡ IN PROGRESS - Workflow executing"
    echo "â„¹ï¸  Increase timeout or check execution progress manually"
else
    if [ "$REAL_DATA_MODE" = true ]; then
        echo "âŒ OVERALL RESULT: âŒ FAILURE - Real data integration failed"
        echo "âš ï¸  Check API keys, network connectivity, and Airflow UI"
    else
        echo "âŒ OVERALL RESULT: âŒ FAILURE - Workflow execution failed"
    fi
    echo "âš ï¸  Check Airflow UI for task execution details"
fi

echo ""
echo "ğŸ”— Access Airflow UI: http://localhost:8081"
echo "   Username: admin / Password: admin"
echo ""

# Exit with appropriate code
if [ "$final_result" == "SUCCESS" ]; then
    exit 0
else
    exit 1
fi