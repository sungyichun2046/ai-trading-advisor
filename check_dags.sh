#!/bin/bash
# ------------------------------------------------------------------
# Test Airflow DAGs Locally (Isolated Environment)
#
# Steps:
#   1. Test local DAG imports
#   2. Stop old test containers
#   3. Start new ones: docker compose -f docker-compose.test.yml up -d
#   4. Wait for Airflow to be ready
#   5. Run DAG tests + report results
#
# Uses apache/airflow:2.7.3-python3.11
# Mounts ./src/dags â†’ /opt/airflow/test_dags
# No image build needed â€” DAGs loaded directly from local folder.
# Uses separate test Docker environment to avoid impacting main Airflow

set -e

echo "ğŸš€ NEW DAG STRUCTURE VALIDATION (TEST ENVIRONMENT)"
echo "================================================="

# Set environment for new DAG folder
export AIRFLOW__CORE__DAGS_FOLDER="$(pwd)/src/dags"
export POSTGRES_HOST=localhost
export POSTGRES_DB=airflow 
export POSTGRES_USER=airflow
export POSTGRES_PASSWORD=airflow
export AIRFLOW_UID=50000

echo "ğŸ“ DAG Folder: $AIRFLOW__CORE__DAGS_FOLDER"
echo "ğŸ³ Using isolated test Docker environment (port 8081)"
echo ""

# Check if new dags folder exists
if [ ! -d "src/dags" ]; then
    echo "âŒ ERROR: src/dags/ folder not found!"
    echo "   Expected new streamlined DAG structure not present"
    exit 1
fi

echo "ğŸ” SCANNING NEW DAG FOLDER"
echo "=========================="

# List Python files in dags folder
dag_files=$(find src/dags -name "*.py" -not -name "__*" 2>/dev/null || echo "")

if [ -z "$dag_files" ]; then
    echo "âŒ ERROR: No Python DAG files found in src/dags/"
    exit 1
fi

echo "ğŸ“‹ Found DAG files:"
for file in $dag_files; do
    echo "   - $file"
done
echo ""

# Expected DAGs for streamlined structure (final goal: 3 DAGs)
expected_dags=("data_collection" "analysis" "trading")
found_dags=()

echo "ğŸ§ª TESTING DAG IMPORTS (LOCAL)"
echo "============================="

# Test each DAG file import locally first
for dag_file in $dag_files; do
    echo "Testing: $dag_file"
    
    # Extract expected DAG name from filename
    dag_name=$(basename "$dag_file" .py | sed 's/_dag$//')
    
    # Test Python import
    python_test=$(python3 -c "
import sys
sys.path.append('$(pwd)')
try:
    module_path = '$dag_file'.replace('/', '.').replace('.py', '')
    exec(f'from {module_path} import dag')
    print(f'âœ… IMPORT SUCCESS: {dag.dag_id} ({len(dag.tasks)} tasks)')
    print(f'   Schedule: {dag.schedule_interval}')
    print(f'   Tasks: {[t.task_id for t in dag.tasks]}')
except Exception as e:
    print(f'âŒ IMPORT ERROR: {e}')
    exit(1)
" 2>&1)

    if [ $? -eq 0 ]; then
        echo "$python_test"
        found_dags+=("$dag_name")
    else
        echo "âŒ FAILED: $dag_file"
        echo "$python_test"
        exit 1
    fi
    echo ""
done

echo "ğŸ“Š DAG STRUCTURE VALIDATION"
echo "==========================="

# Check current progress toward streamlined structure
echo "Target streamlined structure: ${expected_dags[*]} (3 total)"
echo "Currently found: ${found_dags[*]}"

# Validate current progress (realistic expectations)
total_found=${#found_dags[@]}
if [ $total_found -eq 3 ]; then
    echo "âœ… PERFECT: Found exactly 3 DAGs (streamlined structure complete!)"
elif [ $total_found -eq 1 ]; then
    echo "âœ… PROGRESS: Found $total_found DAG (1/3 streamlined structure)"
    echo "   â†’ data_collection_dag.py successfully created"
    echo "   â†’ Next: analysis_dag.py and trading_dag.py"
elif [ $total_found -eq 2 ]; then
    echo "âœ… PROGRESS: Found $total_found DAGs (2/3 streamlined structure)"
    echo "   â†’ Almost complete! One more DAG needed"
elif [ $total_found -gt 3 ]; then
    echo "âš ï¸  INFO: $total_found DAGs found (more than target of 3)"
    echo "   â†’ Consider consolidating additional DAGs"
else
    echo "âŒ ERROR: No valid DAGs found"
    exit 1
fi
echo ""

echo "ğŸ³ STARTING TEST DOCKER ENVIRONMENT"
echo "=================================="

echo "ğŸ›‘ Completely cleaning test environment..."
docker compose -f docker-compose.test.yml down --volumes --remove-orphans 2>/dev/null || true

# Remove any leftover test volumes to ensure fresh start
echo "ğŸ§¹ Removing any leftover test volumes..."
docker volume rm ai-trading-advisor_test_postgres_data 2>/dev/null || true
docker volume rm ai-trading-advisor_test_airflow_logs 2>/dev/null || true
docker volume rm ai-trading-advisor_test_airflow_plugins 2>/dev/null || true

# Remove any test containers that might be lingering
echo "ğŸ—‘ï¸  Removing any test containers..."
docker container rm ai-trading-advisor-test-postgres-1 2>/dev/null || true
docker container rm ai-trading-advisor-test-airflow-webserver-1 2>/dev/null || true
docker container rm ai-trading-advisor-test-airflow-scheduler-1 2>/dev/null || true
docker container rm ai-trading-advisor-test-airflow-init-1 2>/dev/null || true

echo "âœ… Test environment completely cleaned"

echo "ğŸš€ Starting isolated test Airflow environment..."
echo "   - Test Airflow UI will be available on port 8081"
echo "   - This won't affect your main Airflow on port 8080"

# Start test environment
docker compose -f docker-compose.test.yml up -d

echo "â³ Waiting for fresh test Airflow to initialize (longer wait for clean start)..."
sleep 90

# Wait for test Airflow to be ready
echo "ğŸ”„ Checking test Airflow health..."
max_attempts=20
attempt=0

while [ $attempt -lt $max_attempts ]; do
    health_check=$(curl -s http://localhost:8081/health 2>/dev/null || echo "failed")
    web_access=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8081 2>/dev/null || echo "000")
    
    # Accept both 200 and 302 as valid (302 is redirect to login page)
    if [[ "$health_check" != "failed" ]] && ([[ "$web_access" == "200" ]] || [[ "$web_access" == "302" ]]); then
        echo "âœ… Test Airflow is ready!"
        echo "   Health endpoint: âœ… http://localhost:8081/health"
        echo "   Web interface: âœ… http://localhost:8081 (HTTP $web_access)"
        break
    fi
    
    attempt=$((attempt + 1))
    echo "   Attempt $attempt/$max_attempts (Health: $health_check, Web: HTTP $web_access)..."
    sleep 10
done

if [ $attempt -eq $max_attempts ]; then
    echo "âŒ Test Airflow not ready after $max_attempts attempts"
    echo "ğŸ” Checking what's wrong..."
    
    # Check if containers are running
    echo "ğŸ“‹ Container status:"
    docker compose -f docker-compose.test.yml ps
    
    # Check web access specifically
    web_status=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8081 2>/dev/null || echo "000")
    echo "ğŸŒ Web access test: HTTP $web_status"
    
    if [[ "$web_status" != "200" ]] && [[ "$web_status" != "302" ]]; then
        echo "âŒ http://localhost:8081 is not accessible (HTTP $web_status)"
        echo "ğŸ” Checking webserver logs..."
        docker compose -f docker-compose.test.yml logs test-airflow-webserver | tail -20
        exit 1
    else
        echo "âš ï¸  Continuing with limited functionality..."
    fi
fi

echo ""
echo "ğŸ”§ Creating default_pool to prevent infinite DAGs..."
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow pools set default_pool 5 "Default pool" > /dev/null 2>&1

echo "â¸ï¸  Pausing DAGs to prevent auto-scheduling during test..."
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags pause data_collection > /dev/null 2>&1
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags pause analysis > /dev/null 2>&1
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags pause trading > /dev/null 2>&1
echo "âœ… All DAGs paused (will trigger manually only)"
echo ""

echo "ğŸ“‹ DAG STATUS IN TEST AIRFLOW"
echo "============================"

# Quick DAG verification (simplified)
echo "ğŸ” Quick DAG verification in test Airflow..."

# Wait for DAGs to be loaded by the scheduler
echo "â³ Waiting for DAGs to be loaded by scheduler..."
sleep 20

# Simple DAG list check
echo "ğŸ“‹ Checking if DAGs are loaded..."
all_dags_output=$(docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags list 2>/dev/null | grep -E "(data_collection|analysis|trading)" || echo "")

if [ -n "$all_dags_output" ]; then
    echo "âœ… DAGs found in Airflow"
else
    echo "âš ï¸  DAGs not yet visible in Airflow (may still be loading)"
fi

echo "ğŸš€ Proceeding to execution testing..."
echo ""

echo "ğŸ¯ VALIDATION SUMMARY"
echo "===================="

# Final validation results
echo "âœ… DAG folder structure: src/dags/ âœ“"
echo "âœ… Python imports: All DAGs load successfully âœ“"  
echo "âœ… DAGs found: $total_found"

# Web access validation
web_final_check=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8081 2>/dev/null || echo "000")
if [[ "$web_final_check" == "200" ]] || [[ "$web_final_check" == "302" ]]; then
    echo "âœ… Web interface access: http://localhost:8081 âœ“"
else
    echo "âŒ Web interface access: http://localhost:8081 (HTTP $web_final_check)"
fi

# Skip complex execution summary - go straight to final results

# Progress assessment
if [ $total_found -eq 3 ]; then
    echo "âœ… Streamlined structure: Complete (3/3 DAGs) âœ“"
elif [ $total_found -eq 1 ]; then
    echo "âœ… Streamlined structure: In progress (1/3 DAGs) âœ“"
elif [ $total_found -eq 2 ]; then
    echo "âœ… Streamlined structure: Almost complete (2/3 DAGs) âœ“"
else
    echo "âŒ Streamlined structure: Incomplete ($total_found DAGs)"
fi

echo "âœ… Test isolation: Main Airflow (port 8080) unaffected âœ“"
echo ""

# Skip complex assessment - go straight to execution testing

echo ""
echo "ğŸ¯ WAITING FOR SUCCESSFUL DAG EXECUTIONS"
echo "========================================"

# Pool already created early in the script
echo "ğŸ”§ Using default_pool created earlier..."

# Quick unpause-trigger-pause cycle to get exactly 1 run per DAG
echo "ğŸ¯ Executing one manual trigger per DAG (unpause â†’ trigger â†’ pause)..."

# Data Collection DAG
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags unpause data_collection > /dev/null 2>&1
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags trigger data_collection > /dev/null 2>&1
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags pause data_collection > /dev/null 2>&1

# Analysis DAG  
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags unpause analysis > /dev/null 2>&1
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags trigger analysis > /dev/null 2>&1
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags pause analysis > /dev/null 2>&1

# Trading DAG (with small delay to ensure trigger registers)
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags unpause trading > /dev/null 2>&1
sleep 2  # Brief pause to ensure unpause registers
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags trigger trading > /dev/null 2>&1
sleep 2  # Brief pause to ensure trigger registers
docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags pause trading > /dev/null 2>&1

echo "âœ… All DAGs triggered once and re-paused"

# Short wait for DAGs to complete (reduced from 2 minutes to 90 seconds)
echo "â³ Waiting 90 seconds for DAGs to complete..."
sleep 90

echo "â° Proceeding to final validation..."

echo ""
echo "ğŸ“Š FINAL SUCCESS SUMMARY"
echo "========================"

# Get final counts - just check that we have at least 1 successful run (which proves DAGs work)
data_success_final=$(docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags list-runs -d data_collection 2>/dev/null | grep -c "success" | tr -d '\r' || echo "0")
analysis_success_final=$(docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags list-runs -d analysis 2>/dev/null | grep -c "success" | tr -d '\r' || echo "0")
trading_success_final=$(docker compose -f docker-compose.test.yml exec test-airflow-webserver airflow dags list-runs -d trading 2>/dev/null | grep -c "success" | tr -d '\r' || echo "0")

# Show simplified counts (1+ means working)
if [ "$data_success_final" -gt 0 ]; then
    data_display="1+ (working âœ…)"
else
    data_display="0 (failed âŒ)"
fi

if [ "$analysis_success_final" -gt 0 ]; then
    analysis_display="1+ (working âœ…)"
else
    analysis_display="0 (failed âŒ)"
fi

if [ "$trading_success_final" -gt 0 ]; then
    trading_display="1+ (working âœ…)"
else
    trading_display="0 (failed âŒ)"
fi

echo "Data Collection: $data_display"
echo "Analysis: $analysis_display"
echo "Trading: $trading_display"

# Determine overall result
if [ "$data_success_final" -gt 0 ] && [ "$analysis_success_final" -gt 0 ] && [ "$trading_success_final" -gt 0 ]; then
    echo ""
    echo "âœ… SUCCESS: All 3 DAGs have successful executions in fresh environment!"
    echo "ğŸ¯ Infinite DAGs issue is FIXED"
    if [ "$data_success_final" -eq 1 ] && [ "$analysis_success_final" -eq 1 ] && [ "$trading_success_final" -eq 1 ]; then
        echo "ğŸ¯ Perfect: Exactly 1 successful run per DAG (manual triggers only)"
    fi
    final_result="SUCCESS"
else
    echo ""
    echo "âŒ FAILURE: Some DAGs still have no successful runs"
    echo "âš ï¸  Infinite DAGs issue persists"
    final_result="FAILURE"
fi

echo ""
echo "==============================================="
echo "ğŸ¯ FINAL DAG VALIDATION REPORT"
echo "==============================================="
echo ""
echo "âœ… Data Collection DAG:     $data_display"
echo "âœ… Analysis DAG:            $analysis_display"  
echo "âœ… Trading DAG:             $trading_display"
echo ""
if [ "$final_result" == "SUCCESS" ]; then
    echo "ğŸ‰ OVERALL RESULT: âœ… SUCCESS - All 3 DAGs working!"
    echo "ğŸ† Streamlined structure (3/3 DAGs) complete and functional"
else
    echo "âŒ OVERALL RESULT: âŒ FAILURE - Some DAGs not working"
    echo "âš ï¸  Check individual DAG status above"
fi
echo ""
echo "==============================================="
echo ""
echo "ğŸ”— Access Test Airflow UI: http://localhost:8081"
echo "   Username: admin / Password: admin"