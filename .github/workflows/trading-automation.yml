name: Trading DAG Automation (PRODUCTION)

on:
  push:
    branches:
      - main  # or your target branch
    paths:
      - 'Dockerfile.airflow'          # Rebuild when Dockerfile changes
      - 'requirements-airflow.txt'    # Rebuild when dependencies change
      - 'src/**'                      # Rebuild when source code changes
      - '.github/workflows/**'        # Rebuild when workflow changes
      - '!README.md'                  # Exclude README changes
      - '!docs/**'                    # Exclude documentation
  # 5 scheduled runs per day during market hours
  #schedule:
  #  - cron: '*/3 * * * *' # every 3 minute, for test
    # Pre-market (4:00 AM ET = 9:00 AM UTC)
    #- cron: '0 9 * * 1-5'
    # Market open (9:30 AM ET = 2:30 PM UTC)  
    #- cron: '30 14 * * 1-5'
    # Mid-day (12:00 PM ET = 5:00 PM UTC)
    #- cron: '0 17 * * 1-5'
    # Market close (4:00 PM ET = 9:00 PM UTC)
    #- cron: '0 21 * * 1-5'
    # After hours (6:00 PM ET = 11:00 PM UTC)
    #- cron: '0 23 * * 1-5'
    
  # Manual trigger
  workflow_dispatch:

# PRODUCTION ENVIRONMENT VARIABLES
# Uses docker-compose.yml with production settings
env:
  PYTHONPATH: ${{ github.workspace }}/src
  ENVIRONMENT: production
  AIRFLOW_PORT: 8080
  POSTGRES_DB: airflow
  USE_REAL_DATA: false
  NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}

jobs:
  trading-workflow:
    name: Execute Trading DAG (PRODUCTION Environment)
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 2  # Needed for changed-files detection

      - name: Check for Docker-related changes
        id: docker-changes
        run: |
          echo "Checking for Docker-related file changes..."
          
          # Check if Dockerfile.airflow or requirements-airflow.txt changed
          DOCKER_CHANGES=$(git diff --name-only HEAD~1 HEAD | grep -E "(Dockerfile\.airflow|requirements-airflow\.txt)" || echo "")
          
          if [ -n "$DOCKER_CHANGES" ]; then
            echo "üîÑ Docker rebuild needed due to changes in:"
            echo "$DOCKER_CHANGES"
            echo "rebuild=true" >> $GITHUB_OUTPUT
          else
            echo "‚úÖ No Docker rebuild needed"
            echo "rebuild=false" >> $GITHUB_OUTPUT
          fi

      - name: Set up Docker Buildx (conditional)
        if: steps.docker-changes.outputs.rebuild == 'true'
        uses: docker/setup-buildx-action@v3

      - name: Build Docker images (conditional)
        if: steps.docker-changes.outputs.rebuild == 'true'
        env:
          DOCKER_BUILDKIT: 1
        run: |
          echo "üî® Building Airflow Docker images due to dependency changes..."
          docker build -f Dockerfile.airflow -t ai-trading-advisor-airflow-webserver:latest .
          docker tag ai-trading-advisor-airflow-webserver:latest ai-trading-advisor-airflow-scheduler:latest
          echo "‚úÖ Docker images built successfully"

      - name: Build Summary
        run: |
          echo "üìã BUILD SUMMARY"
          echo "==============="
          echo "Docker Rebuild Required: ${{ steps.docker-changes.outputs.rebuild }}"
          if [ "${{ steps.docker-changes.outputs.rebuild }}" = "true" ]; then
            echo "üî® Fresh Docker images built due to dependency changes"
            echo "üì¶ Images tagged for webserver and scheduler"
            echo "üí° Next run will skip rebuild if no dependency changes"
          else
            echo "‚ôªÔ∏è Using existing Docker images (no dependency changes)"
            echo "üíö Saved ~2-3 minutes by skipping rebuild"
          fi

      - name: Validate environment variables
        run: |
          echo "Validating environment variables..."
          echo "ENVIRONMENT: ${{ env.ENVIRONMENT }}"
          
          if [ -n "${{ secrets.NEWSAPI_KEY }}" ]; then
            echo "NEWSAPI_KEY secret is configured for intelligent caching"
          else
            echo "WARNING: NEWSAPI_KEY not configured - sentiment data will use fallbacks"
          fi
          
          echo "Intelligent caching system will handle all API calls with fallbacks"

      - name: Start Airflow services (PRODUCTION)
        env:
          AIRFLOW_PORT: ${{ env.AIRFLOW_PORT }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
          USE_REAL_DATA: ${{ env.USE_REAL_DATA }}
          NEWSAPI_KEY: ${{ env.NEWSAPI_KEY }}
          ENVIRONMENT: ${{ env.ENVIRONMENT }}
          DOCKER_REBUILD: ${{ steps.docker-changes.outputs.rebuild }}
        run: |
          echo "üè≠ Starting PRODUCTION Airflow services..."
          echo "Port: $AIRFLOW_PORT"
          echo "Database: $POSTGRES_DB"
          echo "USE_REAL_DATA: $USE_REAL_DATA"
          echo "ENVIRONMENT: $ENVIRONMENT"
          echo "Docker Rebuild: $DOCKER_REBUILD"
          
          # Export environment variables for docker-compose.yml
          export AIRFLOW_PORT=$AIRFLOW_PORT
          export POSTGRES_DB=$POSTGRES_DB
          export USE_REAL_DATA=$USE_REAL_DATA
          export NEWSAPI_KEY=$NEWSAPI_KEY
          export ENVIRONMENT=$ENVIRONMENT
          
          # Start supporting services first
          docker compose up -d postgres redis
          
          # Wait for databases to be ready
          echo "Waiting for databases..."
          timeout 60s bash -c 'until docker compose exec -T postgres pg_isready -U airflow; do sleep 2; done'
          
          # Start Airflow services (will use rebuilt images if available)
          if [ "$DOCKER_REBUILD" = "true" ]; then
            echo "üîÑ Starting Airflow with freshly built images..."
            docker compose up -d --force-recreate airflow-webserver airflow-scheduler
          else
            echo "‚ôªÔ∏è Starting Airflow with existing images..."
            docker compose up -d airflow-webserver airflow-scheduler
          fi
          
          # Wait for Airflow to be ready
          echo "Waiting for Airflow..."
          timeout 120s bash -c 'until docker compose exec -T airflow-webserver airflow version; do sleep 5; done'

      - name: Debug and fix DAG loading
        run: |
          echo "Checking DAG folder contents..."
          docker compose exec -T airflow-webserver ls -la /opt/airflow/dags/
          
          echo "Testing DAG import without compilation..."
          docker compose exec -T airflow-webserver python -c "
          import sys; sys.path.insert(0, '/opt/airflow');
          try:
            from dags.trading_dag import dag
            print(f'‚úÖ DAG imported successfully: {dag.dag_id}')
            print(f'DAG tags: {dag.tags}')
            print(f'DAG description: {dag.description}')
            print(f'Task count: {len(dag.tasks)}')
          except Exception as e:
            print(f'‚ùå Import error: {e}')
            import traceback
            traceback.print_exc()
            raise
          "
          
          echo "Forcing DAG refresh..."
          docker compose exec -T airflow-webserver airflow dags reserialize
          
          echo "Waiting for DAG to appear in list..."
          timeout 90s bash -c 'until docker compose exec -T airflow-webserver airflow dags list | grep -q "trading_workflow"; do sleep 5; echo "Still waiting for DAG..."; done'
          
          echo "Unpausing DAG..."
          docker compose exec -T airflow-webserver airflow dags unpause trading_workflow || echo "Unpause failed"
          
          echo "Final DAG verification..."
          docker compose exec -T airflow-webserver airflow dags list | grep trading_workflow || echo "DAG still not found after all steps"

      - name: Trigger airflow dag (PRODUCTION)
        env:
          AIRFLOW_PORT: ${{ env.AIRFLOW_PORT }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
          USE_REAL_DATA: ${{ env.USE_REAL_DATA }}
          NEWSAPI_KEY: ${{ env.NEWSAPI_KEY }}
          ENVIRONMENT: ${{ env.ENVIRONMENT }}
        run: |
          execution_date=$(date -u +"%Y-%m-%dT%H:%M:%S")
          echo "üè≠ Triggering PRODUCTION workflow at: $execution_date"
          echo "Port: $AIRFLOW_PORT, Database: $POSTGRES_DB"
          echo "USE_REAL_DATA: $USE_REAL_DATA, ENVIRONMENT: $ENVIRONMENT"
          
          # Export environment variables for docker-compose.yml
          export AIRFLOW_PORT=$AIRFLOW_PORT
          export POSTGRES_DB=$POSTGRES_DB
          export USE_REAL_DATA=$USE_REAL_DATA
          export NEWSAPI_KEY=$NEWSAPI_KEY
          export ENVIRONMENT=$ENVIRONMENT
          
          docker compose exec -T airflow-webserver \
            airflow dags trigger trading_workflow -e "$execution_date"
          echo "‚úÖ DAG triggered successfully on port 8080"

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: airflow-logs-${{ github.run_id }}
          path: |
            logs/
            /var/log/
          retention-days: 7
        continue-on-error: true

      - name: Cleanup
        if: always()
        run: |
          echo "üßπ Cleaning up..."
          docker compose down -v || true
          docker system prune -f || true

concurrency:
  group: trading-automation-${{ github.ref }}
  cancel-in-progress: false