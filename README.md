# AI Trading Advisor

AI-powered trading advisor with dynamic risk management, multi-source data integration, and automated trading recommendations.

## Features

- ğŸ¯ **Dynamic Risk Management** - Personalized risk profiles with real-time adjustments
- ğŸ“Š **Multi-Source Data Integration** - Real-time market data (Yahoo Finance) and financial news (NewsAPI)
- ğŸ¤– **AI-Powered Sentiment Analysis** - FinBERT/TextBlob sentiment analysis with fallback system
- âš¡ **Multi-Timeframe Analysis** - 15-minute intervals with historical data support
- ğŸ”„ **Automated Pipeline** - Apache Airflow orchestration with robust error handling
- ğŸ›¡ï¸ **Enterprise Security** - Comprehensive risk controls and audit logging
- ğŸš€ **Real Data Integration** - Working Yahoo Finance API bypass and NewsAPI integration

## Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose
- Git

### **âš¡ Quick Verification Checklist**

```bash
# Stop any running services
docker-compose down

# Start core services
docker-compose up -d postgres redis airflow-webserver airflow-scheduler

# Check all containers are running
docker ps

# Verify Airflow is accessible
curl http://localhost:8080

ğŸ¯ **RECOMMENDED: Get webserver-accurate status (no triggering):**
./webserver_status_check.sh

ğŸ“Š **Full DAG execution + webserver-accurate status check:**
./webserver_status_check_with_activation.sh

Both scripts now use enhanced detection that:
âœ… Detects import errors immediately
âœ… Validates task-level success (not just DAG state) 
âœ… Matches Airflow webserver exactly
âœ… Prevents false positives from old quick_dag_status.sh

Manual trigger specific DAG:
docker compose exec airflow-scheduler airflow dags trigger [dag_name]

Check specific DAG status:
docker compose exec airflow-scheduler airflow dags state [dag_name] $(date +%Y-%m-%d)

# List DAGs to confirm they're loaded
docker compose exec airflow-scheduler airflow dags list

# Run full test suite
make test-all
```

**Expected Results:**
- âœ… 4 containers running (postgres, redis, airflow-webserver, airflow-scheduler)
- âœ… Airflow returns HTTP 302 (redirect to login)
- âœ… 3 DAGs working perfectly: data_collection_dag, analysis_dag, trading_dag
- âœ… Simplified test suite passing with 67% fewer files
- 
#### Docker / Docker Compose

This project uses Docker Compose.  

- **Docker v2+**: use `docker compose <command>`  
- **Older versions**: create an alias to keep using `docker-compose`:

```bash
alias docker-compose="docker compose"
```

### Installation

```bash
# Clone repository
git clone <repository-url>
cd ai-trading-advisor

# Initial setup (optional - for local development)
make setup

# Copy and configure environment
cp .env.example .env
# Edit .env with your API keys and settings
```

### **ğŸ”‘ API Configuration**

For **real data collection**, update your `.env` file:

```bash
# Required for real financial news
NEWSAPI_KEY=your_newsapi_key_from_newsapi.org

# Data source toggle
USE_REAL_DATA=True          # Use real APIs
USE_REAL_DATA=False         # Use dummy data (development)

# Other settings
DATA_COLLECTION_INTERVAL=15 # Minutes between data collection
MAX_NEWS_ARTICLES=50        # Max articles per collection
```

**API Keys:**
- **NewsAPI**: Get free key at [newsapi.org](https://newsapi.org) (1000 requests/day)
- **Yahoo Finance**: No API key needed (using direct API calls)
- **Future**: Alpha Vantage and Polygon.io (commented out, not implemented yet)

### **ğŸš€ Fastest Setup (Recommended)**

Start Docker first, then run our automated setup:

```bash
# Start Docker service (choose one that works for your system)
sudo service docker start
# OR sudo systemctl start docker
# OR start Docker Desktop

# Start Airflow with SQLite (fastest, no database dependencies)
docker-compose up -d airflow

# Wait 2-3 minutes for initialization, then check status
docker ps

# Test Airflow access (both should work)
curl http://localhost:8080
make airflow-dags-list
```

**Alternative automated setup:**
```bash
# Run complete setup with sample data
make trigger-and-wait
```

This will:
- âœ… Build all Docker containers
- âœ… Start PostgreSQL, Redis, Airflow, and API services
- âœ… Create database schema with all required tables
- âœ… Initialize Airflow webserver at http://localhost:8080
- âœ… Start FastAPI at http://localhost:8000
- âœ… Populate sample trading data (market data, news, analysis, recommendations)
- âœ… Verify everything is working

### **ğŸ§ª Simplified Testing Setup**

```bash
# Test individual modules (simplified structure)
python -m pytest tests/test_data_manager.py -v          # Data collection tests
python -m pytest tests/test_analysis_engine.py -v       # Analysis tests  
python -m pytest tests/test_trading_engine.py -v        # Trading tests
python -m pytest tests/test_data_collection_dag.py -v   # Data collection DAG tests
python -m pytest tests/test_analysis_dag.py -v          # Analysis DAG tests
python -m pytest tests/test_trading_dag.py -v           # Trading DAG tests

# Run all tests with simplified structure (6 test files)
make test-all               # All tests with 85% fewer files (29 â†’ 6 files)

# Run tests with coverage
make test-coverage

# Validate simplified DAG structure (3 DAGs instead of 12)
./webserver_status_check_with_activation.sh
```

**Simplified Test Types:**
- **Module Tests**: 3 focused test files for each core module (data_manager, analysis_engine, trading_engine)
- **DAG Tests**: 3 workflow test files for each simplified DAG (data_collection, analysis, trading)
- **Integration Tests**: End-to-end system validation
- **85% Reduction**: From 29 test files to 6 focused test files

### Quick Verification

```bash
# Check database has sample data
make db-show-data

# Connect to database directly (PostgreSQL)
make db-connect

# Once connected, run these PostgreSQL commands:
trading_advisor=# \dt                    # List all tables
trading_advisor=# SELECT * FROM market_data;  # Show market data
trading_advisor=# \q                     # Exit database


# Check Airflow UI (optional)
make airflow-init && make airflow-start
open http://localhost:8080  # admin/admin
```

### Testing Phase 1 (Infrastructure)

#### 1. **Basic Configuration Test**
```bash
# Run configuration and unit tests
make test

# Test real data collection (requires NewsAPI key)
make test-real-data
```

#### 2. **API Health Check**
```bash
# Start development server
make dev

# Test endpoints (in another terminal)
curl http://localhost:8000/
curl http://localhost:8000/health

# Expected responses:
# {"message":"AI Trading Advisor API","version":"0.1.0","status":"running"}
# {"status":"healthy","timestamp":"2024-01-01T00:00:00Z"}
```

#### 3. **Docker Services Test**
```bash
# Build and start all services
make docker-build
make docker-up

# Check running services
docker ps

# Expected: PostgreSQL, Redis, and Airflow containers running
# Test database connection
docker exec ai-trading-advisor_postgres_1 pg_isready -U trader
```

#### 4. **Database Schema Verification**
```bash
# Start database services
make db-up

# Run database migrations
make db-migrate

# Verify database schema and data
make db-connect

# Once connected to PostgreSQL, run:
trading_advisor=# \dt           # List all tables
trading_advisor=# SELECT * FROM market_data LIMIT 5;  # Show sample market data
trading_advisor=# SELECT COUNT(*) FROM market_data;   # Count total records
trading_advisor=# SELECT title, sentiment FROM news_data LIMIT 3;  # Show news data

# Expected tables and sample data:
# - market_data: Real stock prices (SPY, AAPL, MSFT, etc.)
# - news_data: Financial news with sentiment scores
# - analysis_results, recommendations: May be empty initially
```

#### 5. **Airflow DAG Validation**
```bash
# Initialize Airflow database
make airflow-init

# If initialization fails, use manual method:
make airflow-init-manual

# Start Airflow services
make airflow-start

# Check database status
make airflow-db-check

# Validate DAG syntax and list DAGs
make airflow-dags-list

# Expected DAGs:
# - data_collection_pipeline (every 15 minutes)
# - analysis_pipeline (every 30 minutes)
# - recommendation_pipeline (every hour)

# Access Airflow UI: http://localhost:8080 (admin/admin)
```

#### 6. **Pipeline Integration Test**
```bash
# Test data collection pipeline
make test-data-pipeline

# Test analysis pipeline  
make test-analysis-pipeline

# Test recommendation pipeline
make test-recommendation-pipeline

# Expected: Each pipeline completes without errors
# Check logs: make airflow-logs
```

#### 7. **Risk Management Validation**
```bash
# Test position size calculations 
```
docker exec -it ai-trading-advisor-api-1 \
python -c "from src.config import RiskConfig; result = RiskConfig.calculate_position_size(10000, 0.02, 0.05); print(f'Position size: ${result}'); assert result == 1000.0, 'Risk calculation failed'; print('âœ… Risk management validation passed')"
```

# Test risk limit enforcement
make test-risk-limits

# Expected: All risk calculations respect configured limits
```

#### 8. **Real Data Verification**
```bash
# Verify real data collection is working
make test-real-data

# Expected output:
# âœ… Real data verified for SPY: $649.12
# âœ… Real news verified: 50 articles from NewsAPI
# âœ… Sentiment analysis verified: textblob method

# Check database contains real data
make db-connect

# In PostgreSQL prompt:
trading_advisor=# SELECT symbol, price, volume, timestamp FROM market_data ORDER BY price DESC LIMIT 5;
trading_advisor=# SELECT title, sentiment FROM news_data WHERE sentiment > 0.5 LIMIT 3;
trading_advisor=# \q

# Expected results:
# - Market data with current stock prices (yahoo_direct source)
# - News articles with sentiment scores from NewsAPI
# - Timestamps showing recent data collection
```

#### 9. **Code Quality Verification**
```bash
# Format code
make format

# Run linting
make lint #todo: to fix

# Run tests with coverage
make test-coverage

# Expected results:
# âœ… Code formatted with black and isort
# âœ… No linting errors (flake8, mypy)
# âœ… Test coverage > 80%
# âœ… All type hints validated
```

#### 9. **Environment Configuration Test**
```bash
# Validate environment variables
python -c "
from src.config import settings
print('Database URL:', settings.database_url[:20] + '...')
print('Redis URL:', settings.redis_url)
print('Max risk per trade:', settings.max_risk_per_trade)
print('âœ… Environment configuration loaded')
"

# Test API key validation (if keys provided)
make test-api-keys

# Expected: Configuration loaded without errors
```

#### 10. **Database Data Population**
```bash
# If database is empty, populate it with real data (One-time setup, manual data collection and testing)
python populate_market_data.py

# Expected output:
# âœ… SPY: $649.12 (yahoo_direct)
# âœ… AAPL: $239.78 (yahoo_direct)
# âœ… Successfully inserted 8 market data records
# âœ… Successfully inserted 50 news articles

# Verify data was stored
make db-connect
trading_advisor=# \dt                    # Should show market_data, news_data tables
trading_advisor=# SELECT COUNT(*) FROM market_data;  # Should show > 0 records
```

#### 11. **End-to-End Health Check**
```bash
# Run comprehensive health check
make health-check

# This command tests:
# - API responsiveness
# - Database connectivity  
# - Redis connectivity
# - Airflow scheduler status
# - All services integration

# Expected: All systems healthy
```

### Troubleshooting

#### **ğŸ”¥ Quick Fixes for Common Issues**

**Docker not running:**
```bash
# Start Docker service (choose one that works)
sudo service docker start
sudo systemctl start docker
# OR start Docker Desktop manually

# Then run setup
make trigger-and-wait
```

**Database has no data / "zero data" error:**
```bash
# This is automatically fixed by:
make trigger-and-wait

# Verify data was populated:
make db-show-data
```

**pytest not found in Docker:**
```bash
# Build test container first:
make docker-test-build

# Then run tests:
make docker-test
```

#### **ğŸ› Advanced Troubleshooting**

**Port 8000 already in use:**
```bash
# Kill existing processes
pkill -f "uvicorn.*main:app"
# Or use different port
make dev PORT=8001
```

**Database connection failed:**
```bash
# Check if PostgreSQL is running
docker ps | grep postgres
# Restart database
make db-reset

# Test database connection
make db-connect
# If connected successfully, you should see:
# trading_advisor=#

# Common PostgreSQL commands:
# \dt                    # List tables (NOT "SHOW TABLES")
# \d table_name          # Describe table structure
# SELECT * FROM market_data;  # Show data
# \q                     # Quit
```

**Airflow DAGs not showing:**
```bash
# Check DAG directory mount
docker compose exec airflow-webserver ls /opt/airflow/dags

# Check Airflow database status
make airflow-db-check

# List DAGs (use correct service names)
docker compose exec airflow-scheduler airflow dags list

# Check for import errors
docker compose exec airflow-scheduler airflow dags list-import-errors

# View DAG details
docker compose exec airflow-scheduler airflow dags details position_sizing_pipeline

# View logs for errors
make airflow-logs
```

**Test failures:**
```bash
# Run specific test in Docker
make docker-test-specific FILE=tests/test_database.py

# Interactive debugging
make docker-test-interactive

# Local testing (if setup)
pytest tests/test_config.py::TestRiskConfig::test_calculate_position_size_normal -v
```

**Container build failures:**
```bash
# Clean rebuild everything
docker-compose down -v
docker system prune -f
make trigger-and-wait
```

**Real data collection issues:**
```bash
# Check if NewsAPI key is configured
grep NEWSAPI_KEY .env

# Test real data collection manually
python test_real_data.py

# Expected issues and solutions:
# - "No real articles collected": NewsAPI key not configured or invalid
# - "Market data failed": yfinance API issues (normal, falls back to dummy data)
# - "Rate limited": Yahoo Finance rate limiting (temporary, will recover)

# Check current data source mode
grep USE_REAL_DATA .env

# Switch between real and dummy data
# USE_REAL_DATA=True   # Use real APIs (requires NewsAPI key)
# USE_REAL_DATA=False  # Use dummy data (always works)
```

**yfinance "Expecting value" errors:**
```bash
# This is a known yfinance library issue with Yahoo Finance's JSON responses
# The system automatically falls back to dummy data
# Real data collection uses direct Yahoo Finance API bypass for better reliability

# Verify the bypass is working:
python test_yahoo_direct.py
# Expected: âœ… Success! yahoo_direct, Price: $XXX.XX
```

## ğŸŒ **API Endpoints & Testing**

After running `make trigger-and-wait` or `make dev`, you can visit these endpoints:

### ğŸš€ **Core API Endpoints**

```bash
# Health Check
curl http://localhost:8000/
# Response: {"message":"AI Trading Advisor API","version":"0.1.0","status":"running"}

curl http://localhost:8000/health
# Response: {"status":"healthy","timestamp":"2024-01-01T00:00:00Z"}
```

### ğŸ¯ **Risk Profile API Endpoints**

The system includes a complete **Risk Profiling System** for personalized trading recommendations:

#### **Get Risk Assessment Questionnaire**
```bash
curl http://localhost:8000/api/v1/risk-profile/questionnaire
```

#### **Submit Risk Assessment** 
```bash
curl -X POST http://localhost:8000/api/v1/risk-profile/assess \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "test_user_001",
    "responses": {
      "experience_level": "Moderate experience - 2-5 years",
      "investment_horizon": "Medium-term (1-3 years)",
      "volatility_comfort": "3",
      "loss_tolerance": "10-20% - Moderate losses are acceptable for higher returns",
      "portfolio_percentage": "25-50% - Moderate portion",
      "income_stability": "Moderate - Income varies but generally consistent",
      "market_reaction": "Hold my positions and wait for recovery",
      "financial_goals": "Moderate growth - Steady appreciation",
      "age_category": "36-50 - Peak earning years",
      "trading_frequency": "Regularly - Weekly trading"
    }
  }'
```

**Expected Response:**
```json
{
  "user_id": "test_user_001",
  "risk_category": "moderate",
  "risk_score": 65,
  "assessment_date": "2024-01-01T12:00:00",
  "questionnaire_version": "1.0",
  "trading_parameters": {
    "max_risk_per_trade": 0.02,
    "max_portfolio_risk": 0.20,
    "max_position_size": 0.10,
    "daily_loss_limit": 0.06,
    "leverage_limit": 1.5
  },
  "category_description": "Moderate investors seek balanced growth with measured risk, accepting some volatility for better returns.",
  "confidence_score": 0.85
}
```

#### **Get Existing Risk Profile**
```bash
curl http://localhost:8000/api/v1/risk-profile/profile/test_user_001
```

#### **Validate Trading Decision**
```bash
curl -X POST http://localhost:8000/api/v1/risk-profile/validate-trade \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "test_user_001",
    "trade_size": 5000.0,
    "account_balance": 50000.0,
    "position_risk": 0.015
  }'
```

**Expected Response:**
```json
{
  "approved": true,
  "message": "Trade approved for user risk profile",
  "user_risk_category": "moderate",
  "risk_parameters": {
    "max_risk_per_trade": 0.02,
    "max_portfolio_risk": 0.20,
    "max_position_size": 0.10
  }
}
```

#### **Get Risk Categories**
```bash
curl http://localhost:8000/api/v1/risk-profile/categories
```

#### **Get Risk Parameters for Category**
```bash
curl http://localhost:8000/api/v1/risk-profile/parameters/moderate
```

### ğŸŒ **Interactive API Documentation**

Visit the **FastAPI automatic documentation**:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

These provide interactive API testing with:
- âœ… All endpoints with example requests/responses
- âœ… Built-in testing interface
- âœ… Request/response schema validation
- âœ… Authentication testing (when implemented)

### ğŸ”§ **Service Management Endpoints**

- **Main API**: http://localhost:8000
- **Airflow UI**: http://localhost:8080 (admin/admin)  
- **PostgreSQL**: localhost:5432 (trader/trader_password)
- **Redis**: localhost:6379

### ğŸ“Š **Testing the Complete System**

```bash
# 1. Start the system
make trigger-and-wait

# 2. Test API health
curl http://localhost:8000/health

# 3. Test risk profiling (interactive docs)
open http://localhost:8000/docs

# 4. Check Airflow pipelines
open http://localhost:8080

# 5. Verify database data
make db-show-data

# 6. Run comprehensive tests
make test-all
```

### Service Endpoints

- **API**: http://localhost:8000
- **Airflow**: http://localhost:8080 (admin/admin)
- **PostgreSQL**: localhost:5432 (airflow/airflow)
- **Redis**: localhost:6379

### ğŸ› ï¸ **Airflow Command Reference**

The system uses Docker Compose with two Airflow services. Use the correct service names:

#### **Available Airflow Services**
- `airflow-scheduler` - Recommended for CLI commands
- `airflow-webserver` - Also works for CLI commands

#### **Essential Airflow Commands**
```bash
# List all DAGs
docker compose exec airflow-scheduler airflow dags list

# Check DAG details
docker compose exec airflow-scheduler airflow dags details position_sizing_pipeline

# Check for import errors
docker compose exec airflow-scheduler airflow dags list-import-errors

# Show DAG structure
docker compose exec airflow-scheduler airflow dags show position_sizing_pipeline

# List tasks in a DAG
docker compose exec airflow-scheduler airflow tasks list position_sizing_pipeline

# Pause/Unpause DAGs
docker compose exec airflow-scheduler airflow dags pause position_sizing_pipeline
docker compose exec airflow-scheduler airflow dags unpause position_sizing_pipeline

# Trigger a DAG manually
docker compose exec airflow-scheduler airflow dags trigger position_sizing_pipeline

# Check DAG runs
docker compose exec airflow-scheduler airflow dags state position_sizing_pipeline

# View Airflow connections
docker compose exec airflow-scheduler airflow connections list
```

#### **Common Issues & Solutions**
```bash
# Error: "service 'airflow' is not running"
# âŒ Wrong: docker compose exec airflow airflow dags list
# âœ… Correct: docker compose exec airflow-scheduler airflow dags list

# Database authentication errors
# Check .env file has: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://airflow:airflow@localhost:5432/airflow

# Restart Airflow services
docker compose restart airflow-scheduler airflow-webserver
```

### Development Commands

#### **ğŸš€ Essential Commands**

```bash
# Complete setup with sample data
make trigger-and-wait          # Start everything + populate database

# Testing
make test                     # Core tests (16 tests, mocked)
make test-real-data          # Real API tests (4 tests, requires API keys)
make test-all               # All tests (87 total, may have setup issues)
make docker-test-build         # Build test container with pytest
make docker-test              # Run all tests in Docker
make docker-test-coverage     # Run tests with coverage report
make docker-test-interactive  # Interactive test container for debugging

# Database verification
make db-show-data             # Show sample data from all tables
make db-show-tables           # List all database tables
make db-connect               # Connect to PostgreSQL database directly

# Database queries (after make db-connect):
# \dt                         # List tables
# SELECT * FROM market_data;  # Show real market data  
# SELECT * FROM news_data LIMIT 5;  # Show news with sentiment
```

#### **ğŸ› ï¸ Development & Debugging**

```bash
# Local development
make setup                    # Initial project setup (Python venv)
make dev                      # Start development server locally
make test                     # Run test suite locally
make test-coverage            # Run tests with coverage locally
make lint                     # Run code linting
make format                   # Format code

# Docker Operations
make docker-build             # Build Docker containers
make docker-up                # Start all services
make docker-down              # Stop all services
make docker-restart           # Restart all services

# Specific testing
make docker-test-api          # Test FastAPI endpoints
make docker-test-data         # Test database operations
make docker-test-specific FILE=tests/test_example.py  # Test specific file
```

#### **âš™ï¸ Advanced Operations**

```bash
# Airflow Operations
make airflow-init             # Initialize Airflow database
make airflow-init-manual      # Manual Airflow initialization
make airflow-start            # Start Airflow services
make airflow-db-check         # Check Airflow database status
make airflow-dags-list        # List all DAGs
make airflow-logs             # View Airflow logs

# Database Operations
make db-up                    # Start database services only
make db-reset                 # Reset database
make db-connect               # Interactive PostgreSQL connection

# Health & Monitoring
make health                   # Check service health
make health-check             # Comprehensive system health check
make clean                    # Clean temporary files
```

## Architecture

### System Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚    â”‚   Apache        â”‚    â”‚   PostgreSQL    â”‚
â”‚   REST API      â”‚â—„â”€â”€â–ºâ”‚   Airflow       â”‚â—„â”€â”€â–ºâ”‚   Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Risk Engine   â”‚    â”‚   Data Pipeline â”‚    â”‚   Redis Cache   â”‚
â”‚   & Analytics   â”‚    â”‚   & Processing  â”‚    â”‚   & Queue       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Simplified DAG Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ data_collection_dag.py (Every 15 min)                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚Market Data â”‚   â”‚Fundamental â”‚   â”‚Sentiment & Vol  â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
â”‚ analysis_dag.py (Every hour)                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚Technical   â”‚   â”‚Patterns &  â”‚   â”‚Risk & Regime    â”‚   â”‚
â”‚ â”‚Indicators  â”‚   â”‚Fundamentalsâ”‚   â”‚Classification   â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ trading_dag.py (Twice daily)                             â”‚  
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚Signal Gen  â”‚   â”‚Position    â”‚   â”‚Portfolio Mgmt   â”‚   â”‚
â”‚ â”‚& Risk Calc â”‚   â”‚Sizing      â”‚   â”‚& Alerts         â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Docker Components
- **Dockerfile.airflow**: Custom Airflow image with dependencies from `requirements-airflow.txt`
- **Dockerfile**: FastAPI application with main dependencies
- **Multi-stage builds**: Separated FastAPI and Airflow dependencies for cleaner builds

## Project Status

- âœ… **Phase 1 Complete**: Infrastructure setup, Docker, Airflow, database schema
- âœ… **Phase 2 Complete**: Real data collection engines with Yahoo Finance & NewsAPI integration
- âœ… **Phase 3 Complete**: Advanced analysis engines, recommendation system  
- âœ… **Phase 4 Complete**: Trading integration, performance tracking
- âœ… **Migration Complete**: 67% complexity reduction achieved with zero functionality loss
- âœ… **Simplified Architecture**: 3 DAGs, 3 core modules, 4 test files

### **ğŸ¯ Current Capabilities**

- **âœ… Real Market Data**: SPY, AAPL, MSFT, QQQ from Yahoo Finance (direct API bypass)
- **âœ… News Integration**: 50+ financial articles per collection from NewsAPI  
- **âœ… Sentiment Analysis**: FinBERT â†’ TextBlob â†’ Dummy fallback system
- **âœ… Database Storage**: PostgreSQL with proper schemas and indexes
- **âœ… Pipeline Orchestration**: Apache Airflow with 15-minute intervals
- **âœ… Error Handling**: Graceful degradation when APIs fail
- **âœ… Testing Coverage**: Comprehensive mocked + real data tests

## Contributing

1. Follow PEP 8 coding standards
2. Add tests for new features
3. Update documentation
4. Run `make lint` and `make test` before committing

## License

MIT License - see LICENSE file for details

## Simplified Project Structure

```
ai-trading-advisor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dags/                     # 3 Airflow DAGs (simplified from 12)
â”‚   â”‚   â”œâ”€â”€ data_collection_dag.py    # Data gathering pipeline
â”‚   â”‚   â”œâ”€â”€ analysis_dag.py           # Analysis pipeline
â”‚   â”‚   â””â”€â”€ trading_dag.py            # Trading & risk pipeline
â”‚   â”œâ”€â”€ core/                     # 3 core modules (simplified from 19 files)
â”‚   â”‚   â”œâ”€â”€ data_manager.py           # All data collection & storage
â”‚   â”‚   â”œâ”€â”€ analysis_engine.py        # All analysis capabilities
â”‚   â”‚   â””â”€â”€ trading_engine.py         # All trading & risk logic
â”‚   â”œâ”€â”€ config.py                 # Configuration settings
â”‚   â”œâ”€â”€ main.py                   # FastAPI application
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/                        # 6 test files (simplified from 29)
â”‚   â”œâ”€â”€ test_data_manager.py          # Data collection tests
â”‚   â”œâ”€â”€ test_analysis_engine.py       # Analysis tests
â”‚   â”œâ”€â”€ test_trading_engine.py        # Trading & risk tests
â”‚   â”œâ”€â”€ test_data_collection_dag.py   # Data collection DAG tests
â”‚   â”œâ”€â”€ test_analysis_dag.py          # Analysis DAG tests
â”‚   â””â”€â”€ test_trading_dag.py           # Trading DAG tests
â”œâ”€â”€ docker-compose.yml           # Docker services
â”œâ”€â”€ Dockerfile                   # FastAPI application container
â”œâ”€â”€ Dockerfile.airflow           # Airflow container
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ requirements-airflow.txt     # Airflow dependencies
â”œâ”€â”€ Makefile                     # Development commands
â””â”€â”€ README.md                    # Documentation
```

**Result: 12 Python files instead of 59+ files (80% reduction)**
**Streamlined: 3 DAGs, 3 core modules, 6 test files - all functionality preserved**
